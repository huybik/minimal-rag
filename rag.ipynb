{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "\n",
    "DOCS_DIR = \"data/docs\"\n",
    "docs = []\n",
    "for filename in os.listdir(DOCS_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(DOCS_DIR, filename)\n",
    "        text = extract_text(file_path)\n",
    "        docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaling Laws for Neural Language Models\\n\\nJared Kaplan ∗\\n\\nJohns Hopkins University, OpenAI\\n\\njaredk@jhu.edu\\n\\nSam McCandlish∗\\n\\nOpenAI\\n\\nsam@openai.com\\n\\nTom Henighan\\n\\nTom B. Brown\\n\\nBenjamin Chess\\n\\nRewon Child\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nhenighan@openai.com\\n\\ntom@openai.com\\n\\nbchess@openai.com\\n\\nrewon@openai.com\\n\\nScott Gray\\n\\nOpenAI\\n\\nAlec Radford\\n\\nOpenAI\\n\\nJeffrey Wu\\n\\nOpenAI\\n\\nDario Amodei\\n\\nOpenAI\\n\\nscott@openai.com\\n\\nalec@openai.com\\n\\njeffwu@openai.com\\n\\ndamodei@openai.com\\n\\nAbstract\\n\\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\\nused for training, with some trends spanning more than seven orders of magnitude. Other\\narchitectural details such as network width or depth have minimal effects within a wide\\nrange. Simple equations govern the dependence of overﬁtting on model/dataset size and the\\ndependence of training speed on model size. These relationships allow us to determine the\\noptimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-\\nefﬁcient, such that optimally compute-efﬁcient training involves training very large models\\non a relatively modest amount of data and stopping signiﬁcantly before convergence.\\n\\n0\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n3\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n1\\n6\\n3\\n8\\n0\\n.\\n1\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n∗Equal contribution.\\n\\nContributions:\\nJared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex-\\nperiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\\nguidance throughout the project.\\n\\n \\n \\n \\n \\n \\n \\n\\x0cContents\\n\\n1 Introduction\\n\\n2 Background and Methods\\n\\n3 Empirical Results and Basic Power Laws\\n\\n4 Charting the Inﬁnite Data Limit and Overﬁtting\\n\\n5 Scaling Laws with Model Size and Training Time\\n\\n6 Optimal Allocation of the Compute Budget\\n\\n7 Related Work\\n\\n8 Discussion\\n\\nAppendices\\n\\nA Summary of Power Laws\\n\\nB Empirical Model of Compute-Efﬁcient Frontier\\n\\nC Caveats\\n\\nD Supplemental Figures\\n\\n1\\n\\nIntroduction\\n\\n2\\n\\n6\\n\\n7\\n\\n10\\n\\n12\\n\\n14\\n\\n18\\n\\n18\\n\\n20\\n\\n20\\n\\n20\\n\\n22\\n\\n23\\n\\nLanguage provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-\\ning tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of\\ndata for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-\\nguage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching\\nhuman-level performance on many speciﬁc tasks [WPN+19], including the composition of coherent multi-\\nparagraph prompted text samples [RWC+19].\\n\\nOne might expect language modeling performance to depend on model architecture, the size of neural models,\\nthe computing power used to train them, and the data available for this training process. In this work we will\\nempirically investigate the dependence of language modeling loss on all of these factors, focusing on the\\nTransformer architecture [VSP+17, LSP+18]. The high ceiling and low ﬂoor for performance on language\\ntasks allows us to study trends over more than seven orders of magnitude in scale.\\n\\nThroughout we will observe precise power-law scalings for performance as a function of training time, con-\\ntext length, dataset size, model size, and compute budget.\\n\\n1.1 Summary\\n\\nOur key ﬁndings for Transformer language models are are as follows:\\n\\n2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the\\n\\npurely empirical data.\\n\\n2\\n\\n\\x0cFigure 1 Language modeling performance improves smoothly as we increase the model size, datasetset\\nsize, and amount of compute2 used for training. For optimal performance all three factors must be scaled\\nup in tandem. Empirical performance has a power-law relationship with each individual factor when not\\nbottlenecked by the other two.\\n\\nPerformance depends strongly on scale, weakly on model shape: Model performance depends most\\nstrongly on scale, which consists of three factors: the number of model parameters N (excluding embed-\\ndings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,\\nperformance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section\\n3)\\n\\nSmooth power laws: Performance has a power-law relationship with each of the three scale factors\\nN, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude\\n(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance\\nmust ﬂatten out eventually before reaching zero loss. (Section 3)\\n\\nUniversality of overﬁtting: Performance improves predictably as long as we scale up N and D in tandem,\\nbut enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The\\nperformance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the\\nmodel size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)\\n\\nUniversality of training: Training curves follow predictable power-laws whose parameters are roughly\\nindependent of the model size. By extrapolating the early part of a training curve, we can roughly predict the\\nloss that would be achieved if we trained for much longer. (Section 5)\\n\\nTransfer improves with test performance: When we evaluate models on text with a different distribution\\nthan they were trained on, the results are strongly correlated to those on the training validation set with\\na roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant\\npenalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)\\n\\nSample efﬁciency: Large models are more sample-efﬁcient than small models, reaching the same level of\\nperformance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).\\n\\nConvergence is inefﬁcient: When working within a ﬁxed compute budget C but without any other restric-\\ntions on the model size N or available data D, we attain optimal performance by training very large models\\nand stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would\\ntherefore be far more sample efﬁcient than one might expect based on training small models to convergence,\\nwith data requirements growing very slowly as D ∼ C 0.27 with training compute. (Section 6)\\n\\nOptimal batch size: The ideal batch size for training these models is roughly a power of the loss only,\\nand continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million\\ntokens at convergence for the largest models we can train. (Section 5.1)\\n\\nTaken together, these results show that language modeling performance improves smoothly and predictably\\nas we appropriately scale up model size, data, and compute. We expect that larger language models will\\nperform better and be more sample efﬁcient than current models.\\n\\n3\\n\\nDataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest Loss\\x0cFigure 2 We show a series of language model training runs, with models ranging in size from 103 to 109\\nparameters (excluding embeddings).\\n\\nFigure 3 As more compute becomes available, we can choose how much to allocate towards training larger\\nmodels, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in\\ncompute. For optimally compute-efﬁcient training, most of the increase should go towards increased model\\nsize. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to\\nincrease parallelism through larger batch sizes, with only a very small increase in serial training time required.\\n\\n1.2 Summary of Scaling Laws\\n\\nThe test loss of a Transformer trained to autoregressively model language can be predicted using a power-law\\nwhen performance is limited by only either the number of non-embedding parameters N , the dataset size D,\\nor the optimally allocated compute budget Cmin (see Figure 1):\\n\\n1. For models with a limited number of parameters, trained to convergence on sufﬁciently large\\n\\ndatasets:\\n\\nL(N ) = (Nc/N )αN ; αN ∼ 0.076, Nc ∼ 8.8 × 1013 (non-embedding parameters)\\n\\n(1.1)\\n\\n2. For large models trained with a limited dataset with early stopping:\\n\\nL(D) = (Dc/D)αD ; αD ∼ 0.095, Dc ∼ 5.4 × 1013 (tokens)\\n\\n(1.2)\\n\\n3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized\\n\\nmodel, and a sufﬁciently small batch size (making optimal3 use of compute):\\n\\nL(Cmin) = (cid:0)C min\\n\\nc\\n\\n/Cmin\\n\\n(cid:1)αmin\\n\\nC ; αmin\\n\\nC ∼ 0.050, C min\\n\\nc ∼ 3.1 × 108 (PF-days)\\n\\n(1.3)\\n\\n3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch\\n\\nsize, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).\\n\\n4\\n\\nLarger models require fewer samples to reach the same performance10864The optimal model size grows smoothly with the loss target and compute budgetLine color indicates\\nnumber of parameters1071091011Tokens ProcessedCompute (PF-days)10-910-610-3100Test LossCompute-eﬃcient training stops far short of convergence103109106103 Params109 Params10864100x Batch Size<10x Serial Steps>1,000,000x Model SizeData requirements\\ngrow relatively slowlyOptimal model size\\nincreases very quicklyMinimum serial steps increases negligibly\\x0cFigure 4 Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model\\nsize N according to Equation (1.5). Right: After an initial transient period, learning curves for all model\\nsizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when\\ntraining at large batch size (details in Section 5.1).\\n\\nThese relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N , and over two\\norders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters\\n(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2\\ntraining set [RWC+19]. The power laws αN, αD, αmin\\nspecify the degree of performance improvement\\nC\\nexpected as we scale up N , D, or Cmin; for example, doubling the number of parameters yields a loss that\\nis smaller by a factor 2−αN = 0.95. The precise numerical values of Nc, C min\\n, and Dc depend on the\\nc\\nvocabulary size and tokenization and hence do not have a fundamental meaning.\\n\\nThe critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also\\nroughly obeys a power law in L:\\n\\nBcrit (L) =\\n\\nB∗\\nL1/αB\\n\\n,\\n\\nB∗ ∼ 2 · 108 tokens, αB ∼ 0.21\\n\\n(1.4)\\n\\nEquation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset\\nαN\\nαD ∼ N 0.74. In fact, we ﬁnd that there is a single equation combining\\nsize sublinearly according to D ∝ N\\n(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:\\n\\nL(N, D) =\\n\\n(cid:19) αN\\nαD\\n\\n(cid:34)(cid:18) Nc\\nN\\n\\n+\\n\\nDc\\nD\\n\\n(cid:35)αD\\n\\n(1.5)\\n\\nwith ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the\\ntrained log-likelihood for other generative modeling tasks.\\n\\nWhen training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after\\nan initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)\\n(cid:18) Nc\\nN\\n\\nL(N, S) =\\n\\n(cid:18) Sc\\n\\n(1.6)\\n\\n(cid:19)αN\\n\\n(cid:19)αS\\n\\n+\\n\\nSmin(S)\\n\\nwhere Sc ≈ 2.1 × 103 and αS ≈ 0.76, and Smin(S) is the minimum possible number of optimization steps\\n(parameter updates) estimated using Equation (5.4).\\n\\nWhen training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the\\nprediction that the optimal model size N , optimal batch size B, optimal number of steps S, and dataset size\\nD should grow as\\n\\nN ∝ C αmin\\n\\nC /αN , B ∝ C αmin\\n\\nC /αB , S ∝ C αmin\\n\\nC /αS , D = B · S\\n\\n(1.7)\\n\\nwith\\n\\nαmin\\n\\n(1.8)\\nwhich closely matches the empirically optimal results N ∝ C 0.73\\nmin . As the\\ncomputational budget C increases, it should be spent primarily on larger models, without dramatic increases\\nin training time or dataset size (see Figure 3). This also implies that as models grow larger, they become\\nincreasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would\\n\\nC = 1/ (1/αS + 1/αB + 1/αN )\\n\\nmin , and S ∝ C 0.03\\n\\nmin , B ∝ C 0.24\\n\\n5\\n\\n1071081091010Tokens in Dataset2.53.03.54.04.5LossLoss vs Model and Dataset SizeParams708M302M85M3M25M393.2K104105Estimated Smin2.42.83.23.64.04.4LossLoss vs Model Size and Training Steps106107108Parameters (non-embed)\\x0cbe maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total\\ncompute as a power law (see Equation (1.3)).\\n\\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their\\nimplications for training time, and a breakdown of our results per token. We also make some brief compar-\\nisons to LSTMs and recurrent Transformers [DGV+18].\\n\\n1.3 Notation\\n\\nWe use the following notation:\\n\\n• L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in\\n\\nsome cases we report the loss for speciﬁc tokens within the context.\\n\\n• N – the number of model parameters, excluding all vocabulary and positional embeddings\\n• C ≈ 6N BS – an estimate of the total non-embedding training compute, where B is the batch size,\\nand S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,\\nwhere one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.\\n\\n• D – the dataset size in tokens\\n• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the\\ncritical batch size provides a roughly optimal compromise between time and compute efﬁciency.\\n• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of\\nthe loss. This is the training compute that would be used if the model were trained at a batch size\\nmuch less than the critical batch size.\\n\\n• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.\\nThis is also the number of training steps that would be used if the model were trained at a batch size\\nmuch greater than the critical batch size.\\n\\n• αX – power-law exponents for the scaling of the loss as L(X) ∝ 1/X αX where X can be any of\\n\\nN, D, C, S, B, C min.\\n\\n2 Background and Methods\\n\\nWe train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized\\nusing byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-\\nsive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal\\nperformance metric. We record the loss on the WebText2 test distribution and on a selection of other text\\ndistributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though\\nwe also train LSTM models and Universal Transformers [DGV+18] for comparison.\\n\\n2.1 Parameter and Compute Scaling of Transformers\\n\\nWe parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-\\nmension of the residual stream), dﬀ (dimension of the intermediate feed-forward layer), dattn (dimension of\\nthe attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input\\ncontext, with nctx = 1024 except where otherwise noted.\\n\\nWe use N to denote the model size, which we deﬁne as the number of non-embedding parameters\\n\\nN ≈ 2dmodelnlayer (2dattn + dﬀ )\\n\\n= 12nlayerd2\\n\\nmodel with the standard\\n\\ndattn = dﬀ /4 = dmodel\\n\\n(2.1)\\n\\nwhere we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters\\nin an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include\\nthese when discussing the ‘model size’ N ; we will see that this produces signiﬁcantly cleaner scaling laws.\\n\\nEvaluating a forward pass of the Transformer involves roughly\\n\\nCforward ≈ 2N + 2nlayernctxdmodel\\nadd-multiply operations, where the factor of two comes from the multiply-accumulate operation used in\\nmatrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.\\n\\n(2.2)\\n\\n6\\n\\n\\x0cOperation\\n\\nEmbed\\n\\nAttention: QKV\\n\\nAttention: Mask\\n\\nAttention: Project\\n\\nFeedforward\\n\\nDe-embed\\n\\nParameters\\n\\nFLOPs per Token\\n\\n(nvocab + nctx) dmodel\\n\\n4dmodel\\n\\nnlayerdmodel3dattn\\n\\n2nlayerdmodel3dattn\\n\\n—\\n\\nnlayerdattndmodel\\n\\nnlayer2dmodeldﬀ\\n\\n—\\n\\n2nlayernctxdattn\\n\\n2nlayerdattndembd\\n\\n2nlayer2dmodeldﬀ\\n\\n2dmodelnvocab\\n\\nTotal (Non-Embedding) N = 2dmodelnlayer (2dattn + dﬀ ) Cforward = 2N + 2nlayernctxdattn\\n\\nTable 1 Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading\\nterms such as nonlinearities, biases, and layer normalization are omitted.\\n\\nFor contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a\\nrelatively small fraction of the total compute. Since we primarily study models where dmodel (cid:29) nctx/12,\\nwe do not include context-dependent terms in our training compute estimate. Accounting for the backwards\\npass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding\\ncompute as C ≈ 6N ﬂoating point operators per training token.\\n\\n2.2 Training Procedures\\n\\nUnless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with\\na batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than\\n1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and\\nschedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of\\nlearning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate\\nschedule with a 3000 step linear warmup followed by a cosine decay to zero.\\n\\n2.3 Datasets\\n\\nWe train our models on an extended version of the WebText dataset described in [RWC+19]. The original\\nWebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at\\nleast 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January\\nto October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether\\npeople found the link interesting or useful. The text of the new links was extracted with the Newspaper3k\\npython library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010\\nwords (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields\\n2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly-\\nprepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection\\nof publicly-available Internet Books.\\n\\n3 Empirical Results and Basic Power Laws\\n\\nTo characterize language model scaling we train a wide variety of models, varying a number of factors\\nincluding:\\n\\n• Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)\\n\\n• Dataset size (ranging from 22 million to 23 billion tokens)\\n\\n• Shape (including depth, width, attention heads, and feed-forward dimension)\\n\\n• Context length (1024 for most runs, though we also experiment with shorter contexts)\\n• Batch size (219 for most runs, but we also vary it to measure the critical batch size)\\n\\n7\\n\\n\\x0cFigure 5 Performance depends very mildly on model shape when the total number of non-embedding\\nparameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences\\nin parameter counts are compensated for by using the ﬁt to L(N ) as a baseline. Aspect ratio in particular can\\nvary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a\\nloss within 3% of the (48, 1600) model used in [RWC+19].\\n\\nFigure 6 Left: When we include embedding parameters, performance appears to depend strongly on the\\nnumber of layers in addition to the number of parameters. Right: When we exclude embedding parameters,\\nthe performance of models with different depths converge to a single trend. Only models with fewer than 2\\nlayers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.\\n\\nIn this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to\\nlater sections.\\n\\n3.1 Approximate Transformer Shape and Hyperparameter Independence\\n\\nTransformer performance depends very weakly on the shape parameters nlayer, nheads, and dﬀ when we hold\\nthe total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed\\nsize while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,\\nwe simultaneously varied dmodel while keeping N ≈ 12nlayerd2\\nmodel ﬁxed. Similarly, to vary dﬀ at ﬁxed\\nmodel size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table\\n1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower\\nmodels, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.\\n\\n3.2 Performance with Non-Embedding Parameter Count N\\n\\nIn Figure 6 we display the performance of a wide variety of models, ranging from small models with shape\\n(nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through\\n(207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-\\nﬁtting (except possibly for the very largest models).\\n\\nAs shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter count N , which can be ﬁt to the\\nﬁrst term of Equation (1.5), so that\\n\\nL(N ) ≈\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n8\\n\\n(3.1)\\n\\nFeed-Forward Ratio (dff / dmodel) 50M ParametersAspect Ratio (dmodel / nlayer)Attention Head Dimension (dmodel / nhead) 25M Parameters10%8%6%4%2%0%Loss IncreaseA wide range of architectures achieve similar performance22% additional compute\\ncompensates for 1% loss increase106107108109Parameters (with embedding)234567Test Loss0 Layer1 Layer2 Layers3 Layers6 Layers>6 Layers103104105106107108109Parameters (non-embedding)234567Test Loss1 Layer2 Layers3 Layers6 Layers>6 Layers\\x0cFigure 7\\n\\nTo observe these trends it is crucial to study performance as a function of N ; if we instead use the total\\nparameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This\\nsuggests that the embedding matrix can be made smaller without impacting performance, as has been seen in\\nrecent work [LCG+19].\\n\\nAlthough these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets\\nis also a power-law in N with nearly identical power, as shown in Figure 8.\\n\\n3.2.1 Comparing to LSTMs and Universal Transformers\\n\\nIn Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter\\ncount N . The LSTMs were trained with the same dataset and context length. We see from these ﬁgures\\nthat the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match\\nthe Transformer performance for later tokens. We present power-law relationships between performance and\\ncontext position Appendix D.5, where increasingly large powers for larger models suggest improved ability\\nto quickly recognize patterns.\\nWe also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure\\n17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N , at the\\ncost of additional compute per-parameter.\\n\\n3.2.2 Generalization Among Data Distributions\\n\\nWe have also tested our models on a set of additional text data distributions. The test loss on these datasets\\nas a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2\\ndataset. We see that the loss on these other data distributions improves smoothly with model size, in direct\\nparallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the\\nin-distribution validation loss, and does not depend on the duration of training or proximity to convergence.\\nWe also observe no dependence on model depth (see Appendix D.8).\\n\\n3.3 Performance with Dataset Size and Compute\\n\\nWe display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute\\nC in Figure 1.\\n\\nFor the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on ﬁxed subsets of the WebText2\\ndataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\\nﬁt with simple power-law\\n\\nL(D) ≈\\n\\n(cid:19)αD\\n\\n(cid:18) Dc\\nD\\n\\n(3.2)\\n\\nin the dataset size. The data and ﬁt appear in Figure 1.\\n\\nThe total amount of non-embedding compute used during training can be estimated as C = 6N BS, where\\nB is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and\\nbackward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model\\n\\n9\\n\\nLSTM plateaus after <100 tokens\\nTransformer improves through the whole context2M200M3M300M54326Token Index in Context103102101Transformers asymptotically outperform LSTMs due to improved use of long contexts3.64.23.02.44.85.4105108106107109Parameters (non-embedding)TransformersLSTMs1 Layer2 Layers4 LayersTest LossPer-token Test LossParameters:400K400K\\x0cFigure 8 Left: Generalization performance to other data distributions improves smoothly with model size,\\nwith only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-\\nalization performance depends only on training distribution performance, and not on the phase of training.\\nWe compare generalization of converged models (points) to that of a single large model (dashed curves) as it\\ntrains.\\n\\nwith the best performance on step S = C\\n6BS . Note that in these results the batch size B remains ﬁxed for\\nall models, which means that these empirical results are not truly optimal. We will account for this in later\\nsections using an adjusted Cmin to produce cleaner trends.\\n\\nThe result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with\\n\\nL(C) ≈\\n\\n(cid:19)αC\\n\\n(cid:18) Cc\\nC\\n\\n(3.3)\\n\\nThe ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.\\nWe will study the optimal allocation of compute more closely later on. The data strongly suggests that sample\\nefﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.\\n\\n4 Charting the Inﬁnite Data Limit and Overﬁtting\\n\\nIn Section 3 we found a number of basic scaling laws for language modeling performance. Here we will\\nstudy the performance of a model of size N trained on a dataset with D tokens while varying N and D\\nsimultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling\\nlaw of Equation (1.5). This provides guidance on how much data we would need to train models of increasing\\nsize while keeping overﬁtting under control.\\n\\n4.1 Proposed L(N, D) Equation\\n\\nWe have chosen the parameterization (1.5) (repeated here for convenience):\\n\\nL(N, D) =\\n\\n(cid:19) αN\\nαD\\n\\n(cid:34)(cid:18) Nc\\nN\\n\\n+\\n\\nDc\\nD\\n\\n(cid:35)αD\\n\\n(4.1)\\n\\nusing three principles:\\n\\n1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The\\nparameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling.\\n2. Fixing D and sending N → ∞, the overall loss should approach L(D). Conversely, ﬁxing N and\\n\\nsending D → ∞ the loss must approach L(N ).\\n\\n3. L(N, D) should be analytic at D = ∞, so that it has a series expansion in 1/D with integer powers.\\n\\nTheoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.\\n\\nOur choice of L(N, D) satisﬁes the ﬁrst requirement because we can rescale Nc, Dc with changes in the\\nvocabulary. This also implies that the values of Nc, Dc have no fundamental meaning.\\n\\n10\\n\\n104105106107108109Parameters (non-embedding)34567Test LossWebText2 (Test)Internet BooksBooksWikipediaCommon Crawl2.53.03.54.04.55.0Test Loss on Training Distribution2.53.03.54.04.55.0Loss on Other DistributionBooks during trainingWikipedia during trainingBooks at convergenceWikipedia at convergence\\x0cFigure 9 The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N\\naccording to Equation (1.5). Left: For large D, performance is a straight power law in N . For a smaller ﬁxed\\nD, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,\\nαN\\nαD /D, as predicted in\\nsee Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N\\nequation (4.3). The line is our ﬁt to that equation.\\n\\nSince we stop training early when the test loss ceases to improve and optimize all models in the same way, we\\nexpect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also\\ndo not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,\\na model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note\\nthat knowledge of L(N ) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N, D).\\n\\nThe third principle is more speculative. There is a simple and general reason one might expect overﬁtting\\nto scale ∝ 1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio\\nof the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,\\nsince we expect to be able to expand the loss about the D → ∞ limit. However, this argument assumes that\\n1/D corrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the\\nefﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.\\n\\nOur third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar\\nsymmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and\\nwould require the introduction of an additional parameter.\\n\\nIn any case, we will see that our equation for L(N, D) ﬁts the data well, which is the most important justiﬁ-\\ncation for our L(N, D) ansatz.\\n\\n4.2 Results\\n\\nWe regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer\\ndecreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN , αD, Nc, Dc in\\nEquation (1.5):\\n\\nParameter\\n\\nαN\\n\\nαD\\n\\nNc\\n\\nDc\\n\\nValue\\n\\n0.076\\n\\n0.103\\n\\n6.4 × 1013\\n\\n1.8 × 1013\\n\\nTable 2 Fits to L(N, D)\\n\\nWe obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of\\n1024, to about 2 × 107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.\\nPerhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very\\nearly in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in\\nSection 3, as here we are ﬁtting the full L(N, D) rather than just L(N, ∞) or L(∞, D).\\n\\nTo chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but\\nthe largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,\\nso we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by\\n\\n4For example, one might have used L(N, D) = (cid:2)(cid:0) Nc\\n\\nN\\n\\n(cid:1)αN + (cid:0) Dc\\n\\nD\\n\\n(cid:1)αD (cid:3)β\\n\\n, but this does not have a 1/D expansion.\\n\\n11\\n\\n106107108109Params (non-embed)2.53.03.54.04.5Test LossData Size BottleneckData Size21M43M86M172M344M688M1.4B22.0B104103102101NN/D/D0.00.10.20.30.40.5L/L(D=)1OverfittingData Size21M43M86M172M344M688M1.4B22.0B\\x0cFigure 10 The critical batch size Bcrit follows a power law in the loss as performance increase, and does\\nnot depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every\\n13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly\\npredicted by the gradient noise scale, as in [MKAT18].\\n\\ndeﬁning\\n\\nδL(N, D) ≡\\n\\nL(N, D)\\nL(N, ∞)\\n\\n− 1\\n\\n(4.2)\\n\\nand studying it as a function of N, D. In fact, we see empirically that δL depends only a speciﬁc combination\\nof N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies\\n\\n(cid:32)\\n\\nδL ≈\\n\\n1 +\\n\\n(cid:33)αD\\n\\n(cid:18) N\\nNc\\n\\n(cid:19) αN\\n\\nαD Dc\\nD\\n\\n− 1\\n\\n(4.3)\\n\\nNote that at large D this formula also has a series expansion in powers of 1/D.\\n\\nWe estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to\\navoid overﬁtting when training to within that threshold of convergence we require\\n\\nD (cid:38) (5 × 103) N 0.74\\n\\n(4.4)\\n\\nWith this relation, models smaller than 109 parameters can be trained with minimal overﬁtting on the 22B\\ntoken WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this\\nrelation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however\\nthat this does not typically represent maximally compute-efﬁcient training. We should also emphasize that\\nwe have not optimized regularization (eg the dropout probability) while varying dataset and model size.\\n\\n5 Scaling Laws with Model Size and Training Time\\n\\nIn this section we will demonstrate that a simple scaling law provides a good description for the loss as a\\nfunction of model size N and training time. First we will explain how to use the results of [MKAT18] to\\ndeﬁne a universal training step Smin, which accounts for the fact that most of our models have not been\\ntrained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time\\ndependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation\\nof training compute between model size and training time, and then conﬁrm that prediction.\\n\\n5.1 Adjustment for Training at Bcrit(L)\\n\\nA simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also\\n[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit\\nthe batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > Bcrit\\nincreases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple\\n\\n12\\n\\n1013×1004×1006×100WebText2 Train Loss103104105106Critical Batch Size (Tokens)Critical Batch Size vs. PerformanceEmpirical Bcrit, N=3MEmpirical Bcrit, N=85MBcrit=2.1×108tokensL4.8Noise Scale Measurement\\x0cprediction for Bcrit, and that neither depends directly on model size except through the value of the loss that\\nhas been attained. These results can be used to predict how training time and compute will vary with the\\nbatch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch\\nsize B ≈ Bcrit. Training at B (cid:29) Bcrit minimizes the number of training steps, while B (cid:28) Bcrit minimizes\\nthe use of compute.\\n\\nMore speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training\\nsteps S and the number of data examples processed E = BS satisfy the simple relation\\n\\n(cid:18) S\\n\\nSmin\\n\\n(cid:19) (cid:18) E\\n\\n− 1\\n\\nEmin\\n\\n(cid:19)\\n\\n− 1\\n\\n= 1\\n\\n(5.1)\\n\\nwhen training to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach\\nL, while Emin is the minimum number of data examples that must be processed.\\n\\nWe demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the\\ncritical batch size\\n\\nwhich is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal\\ntime/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.\\nIn Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for\\ntwo different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So\\nthe predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can\\nbe ﬁt with a power-law in the loss\\n\\nBcrit(L) ≡\\n\\nEmin\\nSmin\\n\\nBcrit(L) ≈\\n\\nB∗\\nL1/αB\\n\\n(5.2)\\n\\n(5.3)\\n\\nwhere B∗ ≈ 2 × 108 and αB ≈ 0.21.\\nWe have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,\\nthe gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not\\nknow Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural\\nlanguage is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used\\na parameterization where Bcrit diverges as L → 0.\\n\\nWe will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch\\nsize B = 219 tokens and the number of training steps while training at B (cid:29) Bcrit. This is simply\\n\\nSmin(S) ≡\\n\\nS\\n1 + Bcrit(L)/B\\n\\n(minimum steps, at B (cid:29) Bcrit)\\n\\n(5.4)\\n\\nfor any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L\\nwith a model of size N if we were to train at B (cid:28) Bcrit(L). This is\\n\\nCmin(C) ≡\\n\\nC\\n1 + B/Bcrit(L)\\n\\n(minimum compute, at B (cid:28) Bcrit)\\n\\n(5.5)\\n\\nwhere C = 6N BS estimates the (non-embedding) compute used at batch size B.\\n\\n5.2 Results for L(N, Smin) and Performance with Model Size and Compute\\n\\nNow we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the\\nloss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training\\nruns using Equation (1.6), repeated here for convenience:\\n\\nL(N, Smin) =\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αS\\n\\n+\\n\\n(cid:18) Sc\\nSmin\\n\\n(5.6)\\n\\nfor the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt\\nto the data with the parameters:\\n\\n5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of\\n\\nBcrit from Figures 18 and 10 for all our later analyses.\\n\\n13\\n\\n\\x0cFigure 11 When we hold either total compute or number of training steps ﬁxed, performance follows\\nL(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that\\nmaximizes performance. Mediocre ﬁts at small S are unsurprising, as the power-law equation for the learning\\ncurves breaks down very early in training.\\n\\nParameter\\n\\nαN\\n\\nαS\\n\\nNc\\n\\nSc\\n\\nValue\\n\\n0.077\\n\\n0.76\\n\\n6.5 × 1013\\n\\n2.1 × 103\\n\\nTable 3 Fits to L(N, S)\\n\\nWith these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe\\nthey are quite compelling given the simplicity of Equation (5.6).\\n\\nThe data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we\\nstudy the test loss as a function of model size while ﬁxing either the total non-embedding compute C used\\nin training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters\\nabove and Equation (5.6).\\n\\nThe power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss\\nlandscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-\\nlaw should provide information about the spectrum of the Hessian of the loss. Its universality suggests that\\nthe Hessian eigenvalue density is roughly independent of model size.\\n\\n5.3 Lower Bound on Early Stopping Step\\n\\nThe results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which\\nearly stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D\\nlearning curves for a given model will be very similar until we reach Smin ≈ Sstop. Thus overﬁtting should\\nbe proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because\\nin reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more\\ntraining steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality\\n\\nSstop(N, D) (cid:38)\\n\\nSc\\n[L(N, D) − L(N, ∞)]1/αS\\n\\n(5.7)\\n\\nwhere L(N, ∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-\\nparison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop\\nand L(N, D) are empirical (though Sstop is adjusted to mimic training at B (cid:29) Bcrit), while L(N, ∞) is\\ncomputed from the ﬁt to L(N, D) evaluated at D = ∞.\\n\\n6 Optimal Allocation of the Compute Budget\\n\\nWe displayed the empirical trend of performance as a function of the computation used during training in\\nthe top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know\\n\\n14\\n\\n104106108Parameters (non-embedding)2345678Test LossPerformance vs Compute Budget105104103102101100PF-dayss106107108109Parameters (non-embedding)2.43.03.64.24.85.4Test LossPerformance vs Steps104105Steps\\x0cFigure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger\\nor smaller models can be trained with minimal additional compute. Right: Models larger than the compute-\\nefﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-\\nlelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the\\npower-law region of the learning curve, after initial transient effects.\\n\\nFigure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a\\nsomewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous\\nlump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks\\nin the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger\\ncompute.\\n\\nthat in fact we could train more efﬁciently6 by training at the batch size Bcrit discussed in Section 5.1.\\nLarge and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,\\nand correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more\\npredictable trends.\\n\\nIn this section we will adjust for this oversight. More importantly, we will use the results of Section 5\\nto determine the optimal allocation of compute between model size N and the quantity of data processed\\nduring training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by\\nusing the equation for L(N, Smin), and we will demonstrate that these methods agree.\\n\\n6.1 Optimal Performance and Allocations\\n\\nLet us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is\\nplotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the\\nnew ﬁt with Cmin is somewhat improved.\\n\\nGiven L(Cmin), it is natural to ask for the optimal model size N (Cmin) that provides the minimal loss with a\\ngiven quantity of training compute. The optimal model size is shown in Figure 14. We observe that N (Cmin)\\n\\n6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the\\n\\nmodel but also on the target value of the loss we wish to achieve, and so is a moving target.\\n\\n15\\n\\nModels between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budgetSmaller models require more steps to train, while larger models require fewerOur framework does not capture early training dynamics108106104102100Compute (PF-days), non-embedding234567Test LossL=(Cmin/2.3108)0.050L=(C/2.0107)0.057\\x0cFigure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N . Optimal\\nmodel size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number\\nof data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.\\nRight: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most\\nof the growth in data examples processed can be used for increased batch sizes.\\n\\ncan be ﬁt very well with a power-law\\n\\nN (Cmin) ∝ (Cmin)0.73.\\n\\n(6.1)\\n\\nIn Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).\\n\\nBy deﬁnition Cmin ≡ 6N BcritS, and so we can use N (Cmin) to extract further results. In particular, since\\nprior ﬁts show B ∝ L−4.8 and L ∝ C −0.05\\nmin . This leads us to conclude\\nthat the optimal number of steps will only grow very slowly with compute, as\\n\\nmin , we can conclude that Bcrit ∝ C 0.24\\n\\nSmin ∝ (Cmin)0.03,\\n\\n(6.2)\\n\\nmatching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results\\nmay even be consistent with an exponent of zero.\\n\\nThus we conclude that as we scale up language modeling with an optimal allocation of computation, we\\nshould predominantly increase the model size N , while simultaneously scaling up the batch size via B ∝\\nBcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively\\nfew optimization steps, additional work on speeding up early training dynamics may be warranted.\\n\\n6.2 Predictions from L(N, Smin)\\n\\nThe results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in\\nSection 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin\\n6N B and then ﬁnd the minimum\\nof the loss as a function of N , while ﬁxing the training compute. We carry out this procedure in detail in\\nAppendix B, where we also provide some additional predictions.\\n\\nFor the loss as a function of training compute, we predict that\\n\\nwhere\\n\\nL(Cmin) =\\n\\n(cid:19)αmin\\n\\nC\\n\\n(cid:18) C min\\nc\\nCmin\\n\\nαmin\\n\\nC ≡\\n\\n1\\n1/αS + 1/αB + 1/αN\\n\\n≈ 0.054\\n\\nin excellent agreement with the exponent of Figure 13. We also predict that\\n\\nN (Cmin) ∝ (Cmin)αmin\\n\\nC /αN ≈ (Cmin)0.71\\n\\n(6.3)\\n\\n(6.4)\\n\\n(6.5)\\n\\nwhich also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive\\nframework for the performance of language modeling.\\n\\n16\\n\\n107105103101Compute (PF-days), non-embedding103105107Parameters (non-embedding)N=(1.3109)C0.73minN=(1.6109)C0.88107105103101Compute (PF-days), excluding embeddings050001000015000StepsSmin (adjusted)Smin=(5.4103)C0.03minS (fixed-batch)\\x0cFigure 15 Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations\\nfor L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection\\nmarks the point before which we expect our predictions to break down. The location of this point is highly\\nsensitive to the precise exponents from our power-law ﬁts.\\n\\n6.3 Contradictions and a Conjecture\\n\\nWe observe no signs of deviation from straight power-law trends at large values of compute, data, or model\\nsize. Our trends must eventually level off, though, since natural language has non-zero entropy.\\n\\nIndeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-\\ndiction. At scales several orders of magnitude above those documented here, the performance predicted by\\nthe L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with\\ncompute. This implies that our scaling laws must break down before this point, but we conjecture that the\\nintersection point has a deeper meaning: it provides an estimate of the point at which Transformer language\\nmodels reach maximal performance.\\n\\nSince the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the\\nperformance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).\\nLet us work this out in more detail.\\n\\nTo keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as\\n\\nD ∝ N 0.74 ∝ C 0.54\\nmin\\n\\n(6.6)\\n\\nwhere we have used the compute-efﬁcient N (Cmin) from Figure 14.\\n\\nLet us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch\\nsize (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as\\n\\nD(Cmin) =\\n\\n2Cmin\\n6N (Cmin)\\n\\n≈ (cid:0)4 × 1010 tokens(cid:1) (Cmin/PF-Day)0.26\\n\\n(6.7)\\n\\nThis is the maximum rate at which the dataset size can productively grow with compute, since it means that\\nwe are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).\\nIt appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if\\nthe training process never re-uses any data!\\n\\nAccording to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the\\nloss should scale as L(D) ∝ D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝\\nC −0.03\\nonce we are data-limited. Once again, we have a contradiction, as this will eventually intersect with\\nmin\\nour prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝ C −0.050\\nThe intersection point of L(D(Cmin)) and L(Cmin) occurs at\\n\\nmin\\n\\n.\\n\\nC ∗ ∼ 104 PF-Days N ∗ ∼ 1012 parameters, D∗ ∼ 1012 tokens, L∗ ∼ 1.7 nats/token\\n\\n(6.8)\\n\\nthough the numerical values are highly uncertain, varying by an order or magnitude in either direction de-\\npending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is\\nthat our scaling laws break down at or before we reach this point, which is still many orders of magnitude\\naway in both compute and model size.\\n\\n17\\n\\nThe intersection point is sensitive to the precise power-law parameters\\x0cOne might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\\nsize beyond N ∗ without qualitatively different data requirements, perhaps this means that once we reach\\nC ∗\\nmin and N ∗, we have extracted all of the reliable information available in natural language data. In this\\ninterpretation, L∗ would provide a rough estimate for the entropy-per-token7 of natural language. In this\\nscenario, we would expect the loss trend to level off at or before L∗.\\n\\nWe can guess at the functional form of L(Cmin) as it levels off by considering a version of our training\\ndataset with added noise. For example, we could append a random string of tokens to each context shown\\nto the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise\\nﬂoor L − Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\\npotentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect\\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and\\nmay be meaningful even if it occurs after the leveling off.\\n\\n7 Related Work\\n\\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\\nnumber of relevant features in the data.\\n\\nSome early [BB01, Goo01] work found power-law scalings between performance and dataset size. More\\nrecent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is\\nperhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of\\ndataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our\\nﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets\\n[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent\\nwork [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an\\nansatz similar to ours.\\n\\nEfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal\\nperformance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that\\nfor language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).\\nBut more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the\\noverall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles\\nof shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width\\nand depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies\\nﬁx computation per data example, which tends to scale in proportion to the number of model parameters,\\nwhereas we investigate scaling with both model size and the quantity of training computation.\\n\\nVarious works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-\\ning a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training\\nmany orders of magnitude beyond typical practice, and in particular does not use early stopping). We do\\nnot observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.\\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\\nHessian spectrum [Pap18, GKX19, GARD18].\\n\\n8 Discussion\\n\\nWe have observed consistent scalings of language model log-likelihood loss with non-embedding parameter\\ncount N , dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and\\n(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.\\nSince scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale.\\n\\n7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.\\n8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of\\n\\nloss on both model and dataset size.\\n\\n18\\n\\n\\x0cWe were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when\\nthese parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\\nof overﬁtting, early stopping step, and data requirements when training large language models. So our scaling\\nrelations go beyond mere observation to provide a predictive framework. One might interpret these relations\\nas analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,\\nindependent of most of the details of its microscopic consituents.\\n\\nIt is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a\\nmaximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to\\ntest these relations on other domains, such as images, audio, and video models, and perhaps also for random\\nnetwork distillation. At this point we do not know which of our results depend on the structure of natural\\nlanguage data, and which are universal.\\nIt would also be exciting to ﬁnd a theoretical framework from\\nwhich the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we\\nhave observed. Such a theory might make it possible to derive other more precise predictions, and provide a\\nsystematic understanding of the limitations of the scaling laws.\\n\\nIn the domain of natural language, it will be important to investigate whether continued improvement on the\\nloss translates into improvement on relevant language tasks. Smooth quantitative change can mask major\\nqualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy\\nprovides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth\\nimprovements in language model loss may hide seemingly qualitative changes in capability.\\n\\nOur results strongly suggest that larger models will continue to perform better, and will also be much more\\nsample efﬁcient than has been previously appreciated. Big models may be more important than big data.\\nIn this context, further investigation into model parallelism is warranted. Deep models can be trained using\\npipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased\\nbatch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization\\n[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity\\n[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through\\nincreased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,\\nit might be possible to remain on the compute-efﬁcient frontier for an entire training run.\\n\\nAcknowledgements\\n\\nWe would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\\nback on drafts of this work.\\n\\n19\\n\\n\\x0cAppendices\\n\\nA Summary of Power Laws\\n\\nFor easier reference, we provide a summary below of the key trends described throughout the paper.\\n\\nParameters Data Compute\\n\\nBatch Size Equation\\n\\nN\\n\\n∞\\n\\nOptimal\\n\\n∞\\n\\nD\\n\\n∞\\n\\n∞\\n\\nEarly Stop\\n\\nC\\n\\nFixed\\n\\nFixed\\n\\nFixed\\n\\nL (N ) = (Nc/N )αN\\nL (D) = (Dc/D)αD\\nL (C) = (Cc/C)αC (naive)\\n\\nNopt\\n\\nDopt\\n\\nCmin\\n\\nN\\n\\nN\\n\\nD\\n\\n∞\\n\\nEarly Stop\\n\\nB (cid:28) Bcrit L (Cmin) = (cid:0)C min\\nc\\n(cid:0) Nc\\nN\\n\\nL (N, D) =\\n\\nFixed\\n\\n(cid:20)\\n\\n(cid:1)αmin\\n\\nC\\n\\n(cid:21)αD\\n\\n/Cmin\\n(cid:1) αN\\nαD + Dc\\nD\\n(cid:16)\\n\\n(cid:1)αN +\\n\\n(cid:17)αS\\n\\nSc\\nSmin(S,B)\\n\\nS steps\\n\\nB\\n\\nL (N, S) = (cid:0) Nc\\n\\nN\\n\\nTable 4\\n\\nThe empirical ﬁtted values for these trends are:\\n\\nPower Law\\n\\nScale (tokenization-dependent)\\n\\nαN = 0.076\\n\\nαD = 0.095\\n\\nNc = 8.8 × 1013 params (non-embed)\\nDc = 5.4 × 1013 tokens\\nCc = 1.6 × 107 PF-days\\n\\nαC = 0.057\\nC = 0.050 C min\\nαmin\\nαB = 0.21\\n\\nαS = 0.76\\n\\nc = 3.1 × 108 PF-days\\n\\nB∗ = 2.1 × 108 tokens\\nSc = 2.1 × 103 steps\\n\\nThe optimal parameters for compute efﬁcient training are given by:\\n\\nTable 5\\n\\nCompute-Efﬁcient Value\\n\\nPower Law Scale\\n\\nNopt = Ne · C pN\\nmin\\n= BeC pB\\nB (cid:28) Bcrit = B∗\\nmin\\nL1/αB\\nSmin = Se · C pS\\nmin (lower bound)\\nDopt = De · C pD\\nmin (1 epoch)\\n\\npN = 0.73\\n\\npB = 0.24\\n\\npS = 0.03\\n\\npD = 0.27\\n\\nNe = 1.3 · 109 params\\nBe = 2.0 · 106 tokens\\nSe = 5.4 · 103 steps\\nDe = 2 · 1010 tokens\\n\\nTable 6\\n\\nB Empirical Model of Compute-Efﬁcient Frontier\\n\\nThroughout this appendix all values of C, S, and αC are adjusted for training at the critical batch size Bcrit.\\nWe have left off the ‘adj’ label to avoid cluttering the notation.\\n\\nB.1 Deﬁning Equations\\n\\nThe power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this\\nappendix, we will derive the optimal performance, model size, and number of training steps as a function of\\n\\n20\\n\\n\\x0cthe compute budget. We start with the Equation (1.6), repeated here for convenience:\\n\\nL (N, S) =\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n+\\n\\n(cid:18) Sc\\nS\\n\\n(cid:19)αS\\n\\n.\\n\\n(B.1)\\n\\nHere, S represents the number of parameter updates when training at the critical batch size [MKAT18],\\nwhich was deﬁned in Equation (5.2)9:\\n\\nB (L) =\\n\\nB∗\\nL1/αB\\n\\n.\\n\\n(B.2)\\n\\nWe would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =\\nC/ (6N B (L)), where C is the number of FLOPs used in the training run:\\n\\nL (N, C) =\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN\\n\\n(cid:18)\\n\\n+\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:19)αS\\n\\n.\\n\\nNow, we set ∂N L(cid:12)\\n\\n(cid:12)C = 0 to ﬁnd the condition for optimality:\\n\\n0 =\\n\\n∂L\\n∂N\\n\\n(cid:12)\\n(cid:12)C\\nαN\\nN\\n\\n= −\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n+\\n\\nαS\\nN\\n(cid:19)αS\\n\\n=⇒\\n\\nαN\\nαS\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN\\n\\n(cid:18)\\n\\n=\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:18)\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:19)αS (cid:18)\\n\\n1 − 5\\n\\n(cid:19)\\n\\nN\\nL (cid:26)\\n\\n(cid:26)∂L\\n(cid:12)\\n(cid:26)\\n(cid:12)C\\n∂N\\n\\nEquation (B.3) and (B.4) together determine the compute-efﬁcient frontier.\\n\\nB.2 Efﬁcient Training\\n\\nNow we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields\\n\\nL (Neﬀ (C) , C) =\\n\\n1 +\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nαN\\nαS\\n\\nL (Neﬀ , ∞) ,\\n\\n(B.3)\\n\\n(B.4)\\n\\n(B.5)\\n\\nwhich implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN\\n≈ 10% above\\nαS\\nthe converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating\\nN yields a power-law dependence of performance on compute:\\n\\nwhere we deﬁned\\n\\nL (C) =\\n\\n(cid:19)αC\\n\\n(cid:18) Cc\\nC\\n\\nαC = 1/ (1/αS + 1/αB + 1/αN ) ≈ 0.052\\n\\nCc = 6NcB∗Sc\\n\\n1 +\\n\\n(cid:18)\\n\\nαN\\nαS\\n\\n(cid:19)1/αS +1/αN (cid:18) αS\\nαN\\n\\n(cid:19)1/αS\\n\\n.\\n\\nSimilarly, we can eliminate L to ﬁnd N (C):\\n\\nN (C)\\nNc\\n\\n=\\n\\n(cid:18) C\\nCc\\n\\n(cid:19)αC /αN (cid:18)\\n\\n(cid:19)1/αN\\n\\n1 +\\n\\nαN\\nαS\\n\\nand\\n\\n(B.6)\\n\\n(B.7)\\n\\n(B.8)\\n\\n(B.9)\\n\\n(cid:18)\\n\\nS (C) =\\n\\nαN\\nαS\\n9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could\\ninstead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is\\nthe averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see\\n[MKAT18]).\\n\\nCc\\n6NcB∗\\n\\n(B.10)\\n\\n1 +\\n\\n(cid:19)−1/αN (cid:18) C\\nCc\\n\\n(cid:19)αC /αS\\n\\n21\\n\\n\\x0cB.3 Comparison to Inefﬁcient\\n\\nTypically, researchers train models until they appear to be close to convergence. In this section, we compare\\nthe efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor\\nf as the percent deviation from the converged loss:\\n\\nL (N, C) = (1 + f ) L (N, ∞) .\\n\\n(B.11)\\n\\nFor compute-efﬁcient training we have f = αN /αS ≈ 10% from the previous section, but researchers\\ntypically use a much smaller value. Here, we choose f (cid:48) = 2% as an estimate. For a ﬁxed value of the loss,\\nwe predict:\\n\\nNf\\nNf (cid:48)\\n\\nSf\\nSf (cid:48)\\n\\nCf\\nCf (cid:48)\\n\\n=\\n\\n=\\n\\n=\\n\\n(cid:18) 1 + f\\n1 + f (cid:48)\\n(cid:32) 1 + 1\\nf\\n1 + 1\\nf (cid:48)\\nSf\\nSf (cid:48)\\n\\nNf\\nNf (cid:48)\\n\\n(cid:19)1/αN\\n\\n≈ 2.7\\n\\n(cid:33)1/αS\\n\\n≈ 0.13\\n\\n≈ 0.35\\n\\n(B.12)\\n\\n(B.13)\\n\\n(B.14)\\n\\nSo that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less\\ncompute to reach the same loss.\\n\\nB.4 Suboptimal Model Sizes\\n\\nWe can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss\\nL with a model of size N :\\n\\n(cid:18)\\n\\nC (N, L) =\\n\\n6B∗Sc\\n\\nN\\nL1/αB\\n\\n(cid:19) (cid:18)\\n\\nL −\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN (cid:19)−1/αS\\n\\n.\\n\\n(B.15)\\n\\nUsing A.6 and A.9, we can eliminate L in favor of Neﬀ (L), the model size which reaches L most efﬁciently.\\nFrom there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal\\nmodel size:\\n\\n(cid:20)\\n\\n(cid:18)\\n\\n(cid:19)αN (cid:19)(cid:21)−1/αS\\n\\nC (N, Neﬀ )\\nC (Neﬀ , Neﬀ )\\n\\nαS\\nαN\\nThe result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a\\n20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A\\nlarger model can be trained the the same level of performance in fewer steps, allowing for more parallelism\\nand faster training if sufﬁcient harware is available (see Figure Y):\\n\\nN\\nNeﬀ\\n\\n(B.16)\\n\\n1 −\\n\\n1 +\\n\\n=\\n\\n.\\n\\n(cid:18) Neﬀ\\nN\\n\\nS (N, Neﬀ )\\nS (Neﬀ , Neﬀ )\\n\\n(cid:20)\\n\\n=\\n\\n1 +\\n\\nαS\\nαN\\n\\n(cid:18)\\n\\n1 −\\n\\n(cid:18) Neﬀ\\nN\\n\\n(cid:19)αN (cid:19)(cid:21)−1/αS\\n\\n.\\n\\n(B.17)\\n\\nA 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation\\nshould not be trusted for very large models, as it is only valid in the power-law region of the learning curve\\nafter initial transient effects.\\n\\nC Caveats\\n\\nIn this section we list some potential caveats to our analysis.\\n\\n• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.\\nThe scaling relations with model size and compute are especially mysterious. It may be possible to\\nunderstand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning\\ncurves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very\\nlarge model size still remains mysterious. Without a theory or a systematic understanding of the\\ncorrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.\\n\\n22\\n\\n\\x0cFigure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of\\noverﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:\\nWe display train and test loss for a series of 300M parameter models trained on different sized dataset sub-\\nsamples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the\\ndegree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest − Ltrain\\n(denoted by a black bar for each run).\\n\\n• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the\\nrange we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between\\ndata parallelism and the number of serial training steps required, which would have a major impact\\non training time.\\n\\n• We did not thoroughly investigate the small data regime, and our ﬁts for L(N, D) were poor for\\nthe smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did\\nnot experiment with regularization and data augmentation. Improvements in these could alter our\\nresults, quantitatively or qualitatively.\\n\\n• We used the estimated training compute C ≈ 6N BS, which did not include contributions propor-\\ntional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the\\nregime of very large nctx, speciﬁcally where nctx (cid:38) 12dmodel.\\n\\n• We tuned learning rates, and we experimented with learning rate schedules. But we may have\\nneglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important\\neffect on scaling.\\n\\n• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,\\nit may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short\\ntraining run (eg due to compute limitations), it may be possible to use a larger learning rate. We did\\nnot experiment with higher learning rates for training runs that did not proceed to convergence.\\n\\nD Supplemental Figures\\n\\nD.1 Early Stopping and Test vs Train\\n\\nIn section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on\\nthe early stopping step. We also show the train and test loss for a given model size when training on different\\nsized datasets.\\n\\nD.2 Universal Transformers\\n\\nWe compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.\\nThese models re-use parameters, and so perform slightly better as a function of N , but slightly worse as a\\nfunction of compute C. We include several different different possibilities for parameter re-use.\\n\\nD.3 Batch Size\\n\\nWe measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate\\nBcrit(L) in ﬁgure 10.\\n\\n23\\n\\n103104105Sc×[L(N,D)L(N,)]1/S103104105SstopEarly Stopping StepData Size21M43M86M172M344M688M1.4B103104105Step23456LossTest LossTrain Loss1081091010Dataset Size (Tokens)\\x0cFigure 17 We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-\\nformers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,\\nbut slightly worse when accounting for reuse and comparing per FLOP.\\n\\nFigure 18 These ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and\\nfor two different Transformer model sizes. These ﬁts were used to measure Bcrit(L) for Figure 10.\\n\\nD.4 Sample Efﬁciency vs Model Size\\n\\nIt is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We\\nprovide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach\\nvarious ﬁxed values of the loss.\\n\\nFigure 19 The number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases\\nprecipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)\\nimproves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model\\nto a very large one.\\n\\n24\\n\\n105106107108109Parameters, including reuse (non-embedding)2.53.03.54.04.5Test Loss2x Reuse4x Reuse8x ReuseNon-recurrent Models105106107108109Parameters (non-embedding)2.53.03.54.04.5Test Loss2x Reuse4x Reuse8x ReuseNon-recurrent Models102103104105Step10610710810910101011Tokens ProcessedBatch Size Scan - 3M Params46810Test Loss101102103104105Step1061081010Tokens ProcessedBatch Size Scan - 85M Params46810Test Loss106107108Parameters (non-embedding)103104105Minimum Steps (Smin)2.53.03.54.04.55.05.5Loss106107108Parameters (non-embedding)10810910101011Minimum Examples (Emin)2.53.03.54.04.55.05.5Loss\\x0cFigure 20 This ﬁgure provides information about the performance per token as a function of model size\\nand training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales\\npredictably as a power-law in T . Right: Test loss per token as a function of training step.\\n\\nFigure 21\\nIn addition to the averaged loss, individual tokens within the 1024-token context also improve\\nsmoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better\\non early tokens, since they can allocate all of their capacity to them.\\n\\nD.5 Context Dependence\\n\\nThe trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.\\nWe see that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst\\ntoken.\\n\\nFixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see\\nFigure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,\\nLT16], or a more general feature of the model architecture and optimization. It provides some suggestion for\\nthe potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge\\nto better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger\\nmodels are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we\\nshow how per-token performance varies for a ﬁxed model as a function of the training step. The model begins\\nby learning short-range information, and only learns longer-range correlations later in training.\\n\\nWe have also included models trained with a tiny context nctx = 8 in order to compare with our longer\\ncontext models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024\\nmodels on very early tokens. This also suggests that further improvements should be possible with much\\nlarger models trained on large contexts.\\n\\nD.6 Learning Rate Schedules and Error Analysis\\n\\nWe experimented with a variety of learning rates and schedules. A host of schedules and resulting test\\nperformances for a small language model are plotted in Figure 22. We conclude that the choice of learning\\nrate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the\\nschedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among\\n\\n25\\n\\n100101102103Token Index345678Per-Token Test Loss4.0+3.2T0.473.4+4.0T0.562.9+4.5T0.562.7+4.9T0.602.4+5.1T0.612.3+5.4T0.62106107108Model Parameters101103105Step246810Test LossPer-token Loss (774M Params)100101102103Token Index104105106107108109Parameters (excl. embedding)3.04.56.07.5Test LossToken 1/1024Token 2/1024Token 4/1024Token 8/1024Token 16/1024Token 64/1024Token 256/1024Token 1024/1024Token 1/8Token 2/8Token 4/8Token 8/8\\x0cFigure 22 We test a variety of learning rate schedules including cosine decay, linear decay, as well as other\\nfaster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we\\ndo not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.\\nWe ﬁnd that, as long as the learning rate is not too small and does not decay too quickly, performance does\\nnot depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging\\nmultiple runs is necessary to validate performance changes smaller than this level.\\n\\nFigure 23 The trend for performance as a function of parameter count, L(N ), is ﬁt better by a power law\\nthan by other functions such as a logarithm at a qualitative level.\\n\\nschedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different\\ntraining runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different\\nrandom seeds is roughly constant in magnitude for different model sizes.\\n\\nWe found that larger models require a smaller learning rate to prevent divergence, while smaller models can\\ntolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:\\n\\nLR(N ) ≈ 0.003239 + −0.0001395 log(N )\\n\\n(D.1)\\n\\nWe expect that this formula could be improved. There may be a dependence on network width, likely set by\\nthe initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that\\nit works sufﬁciently well for the models we considered.\\n\\nD.7 Fit Details and Power Law Quality\\n\\nWe experimented with a number of functional forms for the ﬁts to L(N ), L(C), and L(D); the power-law\\nﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).\\n\\nFor L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers\\ncauses a noticable lump in the data. For L(N ) we also do not include very small models with only 1 layer in\\nthe ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change\\nmarginally if we do include them, and the trend extrapolates well in both directions regardless.\\n\\nD.8 Generalization and Architecture\\n\\nIn ﬁgure 24 we show that generalization to other data distributions does not depend on network depth when we\\nhold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.\\n\\n26\\n\\n050000100000150000200000250000Step0.00000.00020.00040.00060.00080.0010Learning Rate50100150200250LR Summed Over Steps3.653.703.753.803.853.90Loss104105106107108109Parameters (non-embedding)23456Test Loss (at convergence)L=(N/8.81013)0.076L=0.25log(N/7.11012)\\x0cFigure 24 We show evaluations on a series of datasets for models with approximately 1.5 Billion param-\\neters. We observe no effect of depth on generalization; generalization performance depends primarily on\\ntraining distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the\\nearly-stopped performance; we have not seen this surprising result in other experiments.\\n\\nList of Figures\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nSummary of simple power laws.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nIllustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .\\n\\nHow to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . .\\n\\nPerformance when varying model and data size, or model and training steps, simultaneously\\n\\n5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . .\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nComparison of performance trend when including or excluding embeddings . . . . . . . . .\\n\\nLSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . .\\n\\nGeneralization to other test datasets\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nUniversality of overﬁtting .\\n\\n10 Critical batch size .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n11\\n\\n12\\n\\nPerformance versus compute budget or number of parameter updates . . . . . . . . . . . . .\\n\\nTraining on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n13 Comparison between empirical and adjusted compute trends\\n\\n. . . . . . . . . . . . . . . . .\\n\\n14 Optimal model size and serial number of steps versus compute budget\\n\\n. . . . . . . . . . . .\\n\\n15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n16\\n\\nEarly stopping lower bound and training curves for overﬁt models\\n\\n. . . . . . . . . . . . . .\\n\\n17 Universal transformers\\n\\n18 Batch size scans .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n19 Another look at sample efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n20\\n\\n21\\n\\n22\\n\\nPower-law dependence of performance on position in context . . . . . . . . . . . . . . . . .\\n\\nPerformance at different context positions versus model size . . . . . . . . . . . . . . . . .\\n\\nLearning rate schedule scan .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n23 Comparison of Power-Law and Logarithmic Fits\\n\\n. . . . . . . . . . . . . . . . . . . . . . .\\n\\n24 Generalization versus depth .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n27\\n\\n3\\n\\n4\\n\\n4\\n\\n5\\n\\n8\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n12\\n\\n14\\n\\n15\\n\\n15\\n\\n16\\n\\n17\\n\\n23\\n\\n24\\n\\n24\\n\\n24\\n\\n25\\n\\n25\\n\\n26\\n\\n26\\n\\n27\\n\\n101102Depth2.32.42.52.62.72.8Test LossWikipediaBooksInternet BooksCommon CrawlWebText2 (Train)WebText2 (Test)\\x0cList of Tables\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nParameter and compute counts for Transformer\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\nFits to L(N, D) .\\n\\nFits to L(N, S) .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\nKey trend equations .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nKey parameters to trend ﬁts .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nTrends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n7\\n\\n11\\n\\n14\\n\\n20\\n\\n20\\n\\n20\\n\\nReferences\\n\\n[ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-\\nrange correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–\\n11587, 2012. 25\\n\\n[AS17]\\n\\n[BB01]\\n\\nMadhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in\\nneural networks. arXiv, 2017, 1710.03667. 11, 18, 22\\n\\nMichele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-\\nbiguation. In Proceedings of the 39th annual meeting on association for computational linguis-\\ntics, pages 26–33. Association for Computational Linguistics, 2001. 18\\n\\n[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine\\n\\nlearning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18\\n\\n[Bia12]\\n\\nGÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,\\n13(Apr):1063–1095, 2012. 18\\n\\n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/\\nabs/1904.10509. 19\\n\\n[DCLT18]\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2\\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-\\nversal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/\\nabs/1807.03819. 6, 9, 23, 24\\n\\n[EP94]\\n\\nWerner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.\\nEPL (Europhysics Letters), 26(4):241, 1994. 25\\n\\n[Fou]\\n\\nThe Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7\\n\\n[GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.\\n\\n2018, arXiv:1812.04754. 18\\n\\n[GJS+19] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,\\nGiulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with\\nnumber of parameters in deep learning. arXiv, 2019, 1901.01608. 18\\n\\n[GKX19]\\n\\n[Goo01]\\n\\n[GRK17]\\n\\n[HAD19]\\n\\nBehrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-\\ntimization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL\\nhttp://arxiv.org/abs/1901.10159. 18\\n\\nJoshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL\\nhttp://arxiv.org/abs/cs.CL/0108005. 18\\n\\nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-\\nnai.com, 2017. 19\\n\\nJoel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-\\ntational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and\\nPractice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.\\ndoi:10.1145/3293883.3295710. 18\\n\\n28\\n\\n\\x0c[HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\\nand Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\\nCoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19\\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\\nninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\\ndictable, empirically, 2017, 1712.00409. 18\\n\\n[JGH18]\\n\\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\\ngeneralization in neural networks. In Advances in neural information processing systems, pages\\n8571–8580, 2018. 18\\n\\n[KB14]\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014,\\n1412.6980. 7\\n\\n[Kom19]\\n\\nAran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18\\n\\n[KSH12]\\n\\nImagenet classiﬁcation with deep\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\\nInformation Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran\\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19\\n\\n[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\\nSoricut. Albert: A lite bert for self-supervised learning of language representations, 2019,\\n1909.11942. 9\\n\\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-\\ning approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/\\n1907.11692. 2\\n\\n[LSP+18]\\n\\n[LT16]\\n\\n[LXS+19]\\n\\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],\\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6\\nHenry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv\\npreprint arXiv:1606.06737, 2016. 25\\n\\nJaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\\nunder gradient descent, 2019, arXiv:1902.06720. 18\\n\\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\\n\\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\\n\\n[Pap18]\\n\\nVardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\\nCoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18\\n\\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\\n\\nImproving language\\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\\nassets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6\\n\\n[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\n\\nprediction of the generalization error across scales, 2019, 1909.12673. 18\\n\\n[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\nprediction of the generalization error across scales, 2019, arXiv:1909.12673. 18\\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer, 2019, arXiv:1910.10683. 2\\n\\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\n\\nmodels are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8\\n\\n[SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\\nBlake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19\\n\\n[SHB15]\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. CoRR, 2015, 1508.07909. 6\\n\\n29\\n\\n\\x0c[SS18]\\n\\n[SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,\\narXiv:1811.03600. 12\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\\ncost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.\\n7\\nStefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.\\nOxford University Press, 2018. 18\\nMingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\\nnetworks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.\\n11946. 18\\n\\n[THK18]\\n\\n[TL19]\\n\\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6\\n\\n[VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles\\n\\n[Was06]\\n\\nof relatively shallow networks, 2016, arXiv:1605.06431. 8, 18\\nLarry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.\\n18\\n\\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\\nOmer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose\\nlanguage understanding systems, 2019, 1905.00537. 2\\n\\n[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-\\ncreasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19\\n\\n[WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional\\n\\nnetworks, 2019, 1906.02909. 19\\n\\n[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\\nXlnet: Generalized autoregressive pretraining for language understanding, 2019,\\n\\nLe.\\narXiv:1906.08237. 2\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British\\nMachine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18\\n\\n[ZK16]\\n\\n[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. 2015 IEEE International Conference on Computer Vision\\n(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7\\n\\n[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,\\nChristopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch\\nsizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL\\nhttp://arxiv.org/abs/1907.04164. 12, 18\\n\\n30\\n\\n\\x0c']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/huy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import nltk\n",
    "import tiktoken\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tiktoken_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sentence_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "\n",
    "def split_by_separator(text, sep):\n",
    "    splits = text.split(sep)\n",
    "    res = [s + sep for s in splits[:-1]]\n",
    "    if splits[-1]:\n",
    "        res.append(splits[-1])\n",
    "    return res\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    spans = [s[0] for s in sentence_tokenizer.span_tokenize(text)] + [len(text)]\n",
    "    return [text[spans[i] : spans[i + 1]] for i in range(len(spans) - 1)]\n",
    "\n",
    "\n",
    "def token_size(text):\n",
    "    return len(tiktoken_tokenizer.encode(text))\n",
    "\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, chunk_size):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.splitters = [\n",
    "            partial(split_by_separator, sep=\"\\n\\n\"),\n",
    "            partial(split_by_separator, sep=\"\\n\"),\n",
    "            split_sentences,\n",
    "            partial(split_by_separator, sep=\" \"),\n",
    "        ]\n",
    "\n",
    "    def _split_recursive(self, text, level=0):\n",
    "        if token_size(text) <= self.chunk_size or level == len(self.splitters):\n",
    "            return [text]\n",
    "\n",
    "        splits = []\n",
    "        for s in self.splitters[level](text):\n",
    "            if token_size(s) <= self.chunk_size:\n",
    "                splits.append(s)\n",
    "            else:\n",
    "                splits.extend(self._split_recursive(s, level + 1))\n",
    "        return splits\n",
    "\n",
    "    def _merge_splits(self, splits):\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for s in splits:\n",
    "            if current_chunk and (token_size(current_chunk + s) > self.chunk_size):\n",
    "                trimmed_chunk = current_chunk.strip()\n",
    "                if trimmed_chunk:\n",
    "                    chunks.append(trimmed_chunk)\n",
    "                current_chunk = \"\"\n",
    "            current_chunk += s\n",
    "\n",
    "        trimmed_chunk = current_chunk.strip()\n",
    "        if trimmed_chunk:\n",
    "            chunks.append(trimmed_chunk)\n",
    "        return chunks\n",
    "\n",
    "    def split(self, text):\n",
    "        splits = self._split_recursive(text)\n",
    "        chunks = self._merge_splits(splits)\n",
    "        return chunks\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "text_splitter = TextSplitter(chunk_size=512)\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_chunks = text_splitter.split(doc)\n",
    "    chunks += doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaling Laws for Neural Language Models\\n\\nJared Kaplan ∗\\n\\nJohns Hopkins University, OpenAI\\n\\njaredk@jhu.edu\\n\\nSam McCandlish∗\\n\\nOpenAI\\n\\nsam@openai.com\\n\\nTom Henighan\\n\\nTom B. Brown\\n\\nBenjamin Chess\\n\\nRewon Child\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nhenighan@openai.com\\n\\ntom@openai.com\\n\\nbchess@openai.com\\n\\nrewon@openai.com\\n\\nScott Gray\\n\\nOpenAI\\n\\nAlec Radford\\n\\nOpenAI\\n\\nJeffrey Wu\\n\\nOpenAI\\n\\nDario Amodei\\n\\nOpenAI\\n\\nscott@openai.com\\n\\nalec@openai.com\\n\\njeffwu@openai.com\\n\\ndamodei@openai.com\\n\\nAbstract\\n\\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\\nused for training, with some trends spanning more than seven orders of magnitude. Other\\narchitectural details such as network width or depth have minimal effects within a wide\\nrange. Simple equations govern the dependence of overﬁtting on model/dataset size and the\\ndependence of training speed on model size. These relationships allow us to determine the\\noptimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-\\nefﬁcient, such that optimally compute-efﬁcient training involves training very large models\\non a relatively modest amount of data and stopping signiﬁcantly before convergence.\\n\\n0\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n3\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n1\\n6\\n3\\n8\\n0\\n.\\n1\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n∗Equal contribution.\\n\\nContributions:\\nJared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex-\\nperiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\\nguidance throughout the project.\\n\\n \\n \\n \\n \\n \\n \\n\\x0cContents\\n\\n1 Introduction\\n\\n2 Background and Methods\\n\\n3 Empirical Results and Basic Power Laws',\n",
       " '4 Charting the Inﬁnite Data Limit and Overﬁtting\\n\\n5 Scaling Laws with Model Size and Training Time\\n\\n6 Optimal Allocation of the Compute Budget\\n\\n7 Related Work\\n\\n8 Discussion\\n\\nAppendices\\n\\nA Summary of Power Laws\\n\\nB Empirical Model of Compute-Efﬁcient Frontier\\n\\nC Caveats\\n\\nD Supplemental Figures\\n\\n1\\n\\nIntroduction\\n\\n2\\n\\n6\\n\\n7\\n\\n10\\n\\n12\\n\\n14\\n\\n18\\n\\n18\\n\\n20\\n\\n20\\n\\n20\\n\\n22\\n\\n23\\n\\nLanguage provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-\\ning tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of\\ndata for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-\\nguage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching\\nhuman-level performance on many speciﬁc tasks [WPN+19], including the composition of coherent multi-\\nparagraph prompted text samples [RWC+19].\\n\\nOne might expect language modeling performance to depend on model architecture, the size of neural models,\\nthe computing power used to train them, and the data available for this training process. In this work we will\\nempirically investigate the dependence of language modeling loss on all of these factors, focusing on the\\nTransformer architecture [VSP+17, LSP+18]. The high ceiling and low ﬂoor for performance on language\\ntasks allows us to study trends over more than seven orders of magnitude in scale.\\n\\nThroughout we will observe precise power-law scalings for performance as a function of training time, con-\\ntext length, dataset size, model size, and compute budget.\\n\\n1.1 Summary\\n\\nOur key ﬁndings for Transformer language models are are as follows:\\n\\n2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the\\n\\npurely empirical data.\\n\\n2\\n\\n\\x0cFigure 1 Language modeling performance improves smoothly as we increase the model size, datasetset\\nsize, and amount of compute2 used for training. For optimal performance all three factors must be scaled\\nup in tandem. Empirical performance has a power-law relationship with each individual factor when not\\nbottlenecked by the other two.',\n",
       " 'Performance depends strongly on scale, weakly on model shape: Model performance depends most\\nstrongly on scale, which consists of three factors: the number of model parameters N (excluding embed-\\ndings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,\\nperformance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section\\n3)\\n\\nSmooth power laws: Performance has a power-law relationship with each of the three scale factors\\nN, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude\\n(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance\\nmust ﬂatten out eventually before reaching zero loss. (Section 3)\\n\\nUniversality of overﬁtting: Performance improves predictably as long as we scale up N and D in tandem,\\nbut enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The\\nperformance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the\\nmodel size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)\\n\\nUniversality of training: Training curves follow predictable power-laws whose parameters are roughly\\nindependent of the model size. By extrapolating the early part of a training curve, we can roughly predict the\\nloss that would be achieved if we trained for much longer. (Section 5)\\n\\nTransfer improves with test performance: When we evaluate models on text with a different distribution\\nthan they were trained on, the results are strongly correlated to those on the training validation set with\\na roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant\\npenalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)\\n\\nSample efﬁciency: Large models are more sample-efﬁcient than small models, reaching the same level of\\nperformance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).',\n",
       " 'Convergence is inefﬁcient: When working within a ﬁxed compute budget C but without any other restric-\\ntions on the model size N or available data D, we attain optimal performance by training very large models\\nand stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would\\ntherefore be far more sample efﬁcient than one might expect based on training small models to convergence,\\nwith data requirements growing very slowly as D ∼ C 0.27 with training compute. (Section 6)\\n\\nOptimal batch size: The ideal batch size for training these models is roughly a power of the loss only,\\nand continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million\\ntokens at convergence for the largest models we can train. (Section 5.1)\\n\\nTaken together, these results show that language modeling performance improves smoothly and predictably\\nas we appropriately scale up model size, data, and compute. We expect that larger language models will\\nperform better and be more sample efﬁcient than current models.\\n\\n3\\n\\nDataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest Loss\\x0cFigure 2 We show a series of language model training runs, with models ranging in size from 103 to 109\\nparameters (excluding embeddings).\\n\\nFigure 3 As more compute becomes available, we can choose how much to allocate towards training larger\\nmodels, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in\\ncompute. For optimally compute-efﬁcient training, most of the increase should go towards increased model\\nsize. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to\\nincrease parallelism through larger batch sizes, with only a very small increase in serial training time required.\\n\\n1.2 Summary of Scaling Laws\\n\\nThe test loss of a Transformer trained to autoregressively model language can be predicted using a power-law\\nwhen performance is limited by only either the number of non-embedding parameters N , the dataset size D,\\nor the optimally allocated compute budget Cmin (see Figure 1):\\n\\n1. For models with a limited number of parameters, trained to convergence on sufﬁciently large\\n\\ndatasets:',\n",
       " 'L(N ) = (Nc/N )αN ; αN ∼ 0.076, Nc ∼ 8.8 × 1013 (non-embedding parameters)\\n\\n(1.1)\\n\\n2. For large models trained with a limited dataset with early stopping:\\n\\nL(D) = (Dc/D)αD ; αD ∼ 0.095, Dc ∼ 5.4 × 1013 (tokens)\\n\\n(1.2)\\n\\n3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized\\n\\nmodel, and a sufﬁciently small batch size (making optimal3 use of compute):\\n\\nL(Cmin) = (cid:0)C min\\n\\nc\\n\\n/Cmin\\n\\n(cid:1)αmin\\n\\nC ; αmin\\n\\nC ∼ 0.050, C min\\n\\nc ∼ 3.1 × 108 (PF-days)\\n\\n(1.3)\\n\\n3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch\\n\\nsize, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).\\n\\n4\\n\\nLarger models require fewer samples to reach the same performance10864The optimal model size grows smoothly with the loss target and compute budgetLine color indicates\\nnumber of parameters1071091011Tokens ProcessedCompute (PF-days)10-910-610-3100Test LossCompute-eﬃcient training stops far short of convergence103109106103 Params109 Params10864100x Batch Size<10x Serial Steps>1,000,000x Model SizeData requirements\\ngrow relatively slowlyOptimal model size\\nincreases very quicklyMinimum serial steps increases negligibly\\x0cFigure 4 Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model\\nsize N according to Equation (1.5). Right: After an initial transient period, learning curves for all model\\nsizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when\\ntraining at large batch size (details in Section 5.1).',\n",
       " 'These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N , and over two\\norders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters\\n(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2\\ntraining set [RWC+19]. The power laws αN, αD, αmin\\nspecify the degree of performance improvement\\nC\\nexpected as we scale up N , D, or Cmin; for example, doubling the number of parameters yields a loss that\\nis smaller by a factor 2−αN = 0.95. The precise numerical values of Nc, C min\\n, and Dc depend on the\\nc\\nvocabulary size and tokenization and hence do not have a fundamental meaning.\\n\\nThe critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also\\nroughly obeys a power law in L:\\n\\nBcrit (L) =\\n\\nB∗\\nL1/αB\\n\\n,\\n\\nB∗ ∼ 2 · 108 tokens, αB ∼ 0.21\\n\\n(1.4)\\n\\nEquation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset\\nαN\\nαD ∼ N 0.74. In fact, we ﬁnd that there is a single equation combining\\nsize sublinearly according to D ∝ N\\n(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:\\n\\nL(N, D) =\\n\\n(cid:19) αN\\nαD\\n\\n(cid:34)(cid:18) Nc\\nN\\n\\n+\\n\\nDc\\nD\\n\\n(cid:35)αD\\n\\n(1.5)\\n\\nwith ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the\\ntrained log-likelihood for other generative modeling tasks.\\n\\nWhen training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after\\nan initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)\\n(cid:18) Nc\\nN',\n",
       " 'L(N, S) =\\n\\n(cid:18) Sc\\n\\n(1.6)\\n\\n(cid:19)αN\\n\\n(cid:19)αS\\n\\n+\\n\\nSmin(S)\\n\\nwhere Sc ≈ 2.1 × 103 and αS ≈ 0.76, and Smin(S) is the minimum possible number of optimization steps\\n(parameter updates) estimated using Equation (5.4).\\n\\nWhen training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the\\nprediction that the optimal model size N , optimal batch size B, optimal number of steps S, and dataset size\\nD should grow as\\n\\nN ∝ C αmin\\n\\nC /αN , B ∝ C αmin\\n\\nC /αB , S ∝ C αmin\\n\\nC /αS , D = B · S\\n\\n(1.7)\\n\\nwith\\n\\nαmin\\n\\n(1.8)\\nwhich closely matches the empirically optimal results N ∝ C 0.73\\nmin . As the\\ncomputational budget C increases, it should be spent primarily on larger models, without dramatic increases\\nin training time or dataset size (see Figure 3). This also implies that as models grow larger, they become\\nincreasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would\\n\\nC = 1/ (1/αS + 1/αB + 1/αN )\\n\\nmin , and S ∝ C 0.03\\n\\nmin , B ∝ C 0.24\\n\\n5\\n\\n1071081091010Tokens in Dataset2.53.03.54.04.5LossLoss vs Model and Dataset SizeParams708M302M85M3M25M393.2K104105Estimated Smin2.42.83.23.64.04.4LossLoss vs Model Size and Training Steps106107108Parameters (non-embed)\\x0cbe maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total\\ncompute as a power law (see Equation (1.3)).\\n\\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their\\nimplications for training time, and a breakdown of our results per token. We also make some brief compar-\\nisons to LSTMs and recurrent Transformers [DGV+18].\\n\\n1.3 Notation',\n",
       " 'We use the following notation:\\n\\n• L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in\\n\\nsome cases we report the loss for speciﬁc tokens within the context.\\n\\n• N – the number of model parameters, excluding all vocabulary and positional embeddings\\n• C ≈ 6N BS – an estimate of the total non-embedding training compute, where B is the batch size,\\nand S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,\\nwhere one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.\\n\\n• D – the dataset size in tokens\\n• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the\\ncritical batch size provides a roughly optimal compromise between time and compute efﬁciency.\\n• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of\\nthe loss. This is the training compute that would be used if the model were trained at a batch size\\nmuch less than the critical batch size.\\n\\n• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.\\nThis is also the number of training steps that would be used if the model were trained at a batch size\\nmuch greater than the critical batch size.\\n\\n• αX – power-law exponents for the scaling of the loss as L(X) ∝ 1/X αX where X can be any of\\n\\nN, D, C, S, B, C min.\\n\\n2 Background and Methods\\n\\nWe train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized\\nusing byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-\\nsive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal\\nperformance metric. We record the loss on the WebText2 test distribution and on a selection of other text\\ndistributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though\\nwe also train LSTM models and Universal Transformers [DGV+18] for comparison.',\n",
       " '2.1 Parameter and Compute Scaling of Transformers\\n\\nWe parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-\\nmension of the residual stream), dﬀ (dimension of the intermediate feed-forward layer), dattn (dimension of\\nthe attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input\\ncontext, with nctx = 1024 except where otherwise noted.\\n\\nWe use N to denote the model size, which we deﬁne as the number of non-embedding parameters\\n\\nN ≈ 2dmodelnlayer (2dattn + dﬀ )\\n\\n= 12nlayerd2\\n\\nmodel with the standard\\n\\ndattn = dﬀ /4 = dmodel\\n\\n(2.1)\\n\\nwhere we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters\\nin an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include\\nthese when discussing the ‘model size’ N ; we will see that this produces signiﬁcantly cleaner scaling laws.\\n\\nEvaluating a forward pass of the Transformer involves roughly\\n\\nCforward ≈ 2N + 2nlayernctxdmodel\\nadd-multiply operations, where the factor of two comes from the multiply-accumulate operation used in\\nmatrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.\\n\\n(2.2)\\n\\n6\\n\\n\\x0cOperation\\n\\nEmbed\\n\\nAttention: QKV\\n\\nAttention: Mask\\n\\nAttention: Project\\n\\nFeedforward\\n\\nDe-embed\\n\\nParameters\\n\\nFLOPs per Token\\n\\n(nvocab + nctx) dmodel\\n\\n4dmodel\\n\\nnlayerdmodel3dattn\\n\\n2nlayerdmodel3dattn\\n\\n—\\n\\nnlayerdattndmodel\\n\\nnlayer2dmodeldﬀ\\n\\n—\\n\\n2nlayernctxdattn\\n\\n2nlayerdattndembd\\n\\n2nlayer2dmodeldﬀ\\n\\n2dmodelnvocab\\n\\nTotal (Non-Embedding) N = 2dmodelnlayer (2dattn + dﬀ ) Cforward = 2N + 2nlayernctxdattn\\n\\nTable 1 Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading\\nterms such as nonlinearities, biases, and layer normalization are omitted.',\n",
       " 'For contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a\\nrelatively small fraction of the total compute. Since we primarily study models where dmodel (cid:29) nctx/12,\\nwe do not include context-dependent terms in our training compute estimate. Accounting for the backwards\\npass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding\\ncompute as C ≈ 6N ﬂoating point operators per training token.\\n\\n2.2 Training Procedures\\n\\nUnless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with\\na batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than\\n1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and\\nschedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of\\nlearning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate\\nschedule with a 3000 step linear warmup followed by a cosine decay to zero.\\n\\n2.3 Datasets\\n\\nWe train our models on an extended version of the WebText dataset described in [RWC+19]. The original\\nWebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at\\nleast 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January\\nto October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether\\npeople found the link interesting or useful. The text of the new links was extracted with the Newspaper3k\\npython library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010\\nwords (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields\\n2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly-\\nprepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection\\nof publicly-available Internet Books.',\n",
       " '3 Empirical Results and Basic Power Laws\\n\\nTo characterize language model scaling we train a wide variety of models, varying a number of factors\\nincluding:\\n\\n• Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)\\n\\n• Dataset size (ranging from 22 million to 23 billion tokens)\\n\\n• Shape (including depth, width, attention heads, and feed-forward dimension)\\n\\n• Context length (1024 for most runs, though we also experiment with shorter contexts)\\n• Batch size (219 for most runs, but we also vary it to measure the critical batch size)\\n\\n7\\n\\n\\x0cFigure 5 Performance depends very mildly on model shape when the total number of non-embedding\\nparameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences\\nin parameter counts are compensated for by using the ﬁt to L(N ) as a baseline. Aspect ratio in particular can\\nvary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a\\nloss within 3% of the (48, 1600) model used in [RWC+19].\\n\\nFigure 6 Left: When we include embedding parameters, performance appears to depend strongly on the\\nnumber of layers in addition to the number of parameters. Right: When we exclude embedding parameters,\\nthe performance of models with different depths converge to a single trend. Only models with fewer than 2\\nlayers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.\\n\\nIn this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to\\nlater sections.\\n\\n3.1 Approximate Transformer Shape and Hyperparameter Independence',\n",
       " 'Transformer performance depends very weakly on the shape parameters nlayer, nheads, and dﬀ when we hold\\nthe total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed\\nsize while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,\\nwe simultaneously varied dmodel while keeping N ≈ 12nlayerd2\\nmodel ﬁxed. Similarly, to vary dﬀ at ﬁxed\\nmodel size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table\\n1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower\\nmodels, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.\\n\\n3.2 Performance with Non-Embedding Parameter Count N\\n\\nIn Figure 6 we display the performance of a wide variety of models, ranging from small models with shape\\n(nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through\\n(207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-\\nﬁtting (except possibly for the very largest models).\\n\\nAs shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter count N , which can be ﬁt to the\\nﬁrst term of Equation (1.5), so that\\n\\nL(N ) ≈\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n8\\n\\n(3.1)\\n\\nFeed-Forward Ratio (dff / dmodel) 50M ParametersAspect Ratio (dmodel / nlayer)Attention Head Dimension (dmodel / nhead) 25M Parameters10%8%6%4%2%0%Loss IncreaseA wide range of architectures achieve similar performance22% additional compute\\ncompensates for 1% loss increase106107108109Parameters (with embedding)234567Test Loss0 Layer1 Layer2 Layers3 Layers6 Layers>6 Layers103104105106107108109Parameters (non-embedding)234567Test Loss1 Layer2 Layers3 Layers6 Layers>6 Layers\\x0cFigure 7',\n",
       " 'To observe these trends it is crucial to study performance as a function of N ; if we instead use the total\\nparameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This\\nsuggests that the embedding matrix can be made smaller without impacting performance, as has been seen in\\nrecent work [LCG+19].\\n\\nAlthough these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets\\nis also a power-law in N with nearly identical power, as shown in Figure 8.\\n\\n3.2.1 Comparing to LSTMs and Universal Transformers\\n\\nIn Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter\\ncount N . The LSTMs were trained with the same dataset and context length. We see from these ﬁgures\\nthat the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match\\nthe Transformer performance for later tokens. We present power-law relationships between performance and\\ncontext position Appendix D.5, where increasingly large powers for larger models suggest improved ability\\nto quickly recognize patterns.\\nWe also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure\\n17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N , at the\\ncost of additional compute per-parameter.\\n\\n3.2.2 Generalization Among Data Distributions\\n\\nWe have also tested our models on a set of additional text data distributions. The test loss on these datasets\\nas a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2\\ndataset. We see that the loss on these other data distributions improves smoothly with model size, in direct\\nparallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the\\nin-distribution validation loss, and does not depend on the duration of training or proximity to convergence.\\nWe also observe no dependence on model depth (see Appendix D.8).\\n\\n3.3 Performance with Dataset Size and Compute\\n\\nWe display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute\\nC in Figure 1.',\n",
       " 'For the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on ﬁxed subsets of the WebText2\\ndataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\\nﬁt with simple power-law\\n\\nL(D) ≈\\n\\n(cid:19)αD\\n\\n(cid:18) Dc\\nD\\n\\n(3.2)\\n\\nin the dataset size. The data and ﬁt appear in Figure 1.\\n\\nThe total amount of non-embedding compute used during training can be estimated as C = 6N BS, where\\nB is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and\\nbackward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model\\n\\n9\\n\\nLSTM plateaus after <100 tokens\\nTransformer improves through the whole context2M200M3M300M54326Token Index in Context103102101Transformers asymptotically outperform LSTMs due to improved use of long contexts3.64.23.02.44.85.4105108106107109Parameters (non-embedding)TransformersLSTMs1 Layer2 Layers4 LayersTest LossPer-token Test LossParameters:400K400K\\x0cFigure 8 Left: Generalization performance to other data distributions improves smoothly with model size,\\nwith only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-\\nalization performance depends only on training distribution performance, and not on the phase of training.\\nWe compare generalization of converged models (points) to that of a single large model (dashed curves) as it\\ntrains.\\n\\nwith the best performance on step S = C\\n6BS . Note that in these results the batch size B remains ﬁxed for\\nall models, which means that these empirical results are not truly optimal. We will account for this in later\\nsections using an adjusted Cmin to produce cleaner trends.\\n\\nThe result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with\\n\\nL(C) ≈\\n\\n(cid:19)αC\\n\\n(cid:18) Cc\\nC\\n\\n(3.3)',\n",
       " 'The ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.\\nWe will study the optimal allocation of compute more closely later on. The data strongly suggests that sample\\nefﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.\\n\\n4 Charting the Inﬁnite Data Limit and Overﬁtting\\n\\nIn Section 3 we found a number of basic scaling laws for language modeling performance. Here we will\\nstudy the performance of a model of size N trained on a dataset with D tokens while varying N and D\\nsimultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling\\nlaw of Equation (1.5). This provides guidance on how much data we would need to train models of increasing\\nsize while keeping overﬁtting under control.\\n\\n4.1 Proposed L(N, D) Equation\\n\\nWe have chosen the parameterization (1.5) (repeated here for convenience):\\n\\nL(N, D) =\\n\\n(cid:19) αN\\nαD\\n\\n(cid:34)(cid:18) Nc\\nN\\n\\n+\\n\\nDc\\nD\\n\\n(cid:35)αD\\n\\n(4.1)\\n\\nusing three principles:\\n\\n1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The\\nparameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling.\\n2. Fixing D and sending N → ∞, the overall loss should approach L(D). Conversely, ﬁxing N and\\n\\nsending D → ∞ the loss must approach L(N ).\\n\\n3. L(N, D) should be analytic at D = ∞, so that it has a series expansion in 1/D with integer powers.\\n\\nTheoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.\\n\\nOur choice of L(N, D) satisﬁes the ﬁrst requirement because we can rescale Nc, Dc with changes in the\\nvocabulary. This also implies that the values of Nc, Dc have no fundamental meaning.\\n\\n10',\n",
       " '104105106107108109Parameters (non-embedding)34567Test LossWebText2 (Test)Internet BooksBooksWikipediaCommon Crawl2.53.03.54.04.55.0Test Loss on Training Distribution2.53.03.54.04.55.0Loss on Other DistributionBooks during trainingWikipedia during trainingBooks at convergenceWikipedia at convergence\\x0cFigure 9 The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N\\naccording to Equation (1.5). Left: For large D, performance is a straight power law in N . For a smaller ﬁxed\\nD, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,\\nαN\\nαD /D, as predicted in\\nsee Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N\\nequation (4.3). The line is our ﬁt to that equation.\\n\\nSince we stop training early when the test loss ceases to improve and optimize all models in the same way, we\\nexpect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also\\ndo not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,\\na model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note\\nthat knowledge of L(N ) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N, D).\\n\\nThe third principle is more speculative. There is a simple and general reason one might expect overﬁtting\\nto scale ∝ 1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio\\nof the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,\\nsince we expect to be able to expand the loss about the D → ∞ limit. However, this argument assumes that\\n1/D corrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the\\nefﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.',\n",
       " 'Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar\\nsymmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and\\nwould require the introduction of an additional parameter.\\n\\nIn any case, we will see that our equation for L(N, D) ﬁts the data well, which is the most important justiﬁ-\\ncation for our L(N, D) ansatz.\\n\\n4.2 Results\\n\\nWe regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer\\ndecreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN , αD, Nc, Dc in\\nEquation (1.5):\\n\\nParameter\\n\\nαN\\n\\nαD\\n\\nNc\\n\\nDc\\n\\nValue\\n\\n0.076\\n\\n0.103\\n\\n6.4 × 1013\\n\\n1.8 × 1013\\n\\nTable 2 Fits to L(N, D)\\n\\nWe obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of\\n1024, to about 2 × 107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.\\nPerhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very\\nearly in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in\\nSection 3, as here we are ﬁtting the full L(N, D) rather than just L(N, ∞) or L(∞, D).\\n\\nTo chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but\\nthe largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,\\nso we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by\\n\\n4For example, one might have used L(N, D) = (cid:2)(cid:0) Nc\\n\\nN\\n\\n(cid:1)αN + (cid:0) Dc\\n\\nD\\n\\n(cid:1)αD (cid:3)β',\n",
       " ', but this does not have a 1/D expansion.\\n\\n11\\n\\n106107108109Params (non-embed)2.53.03.54.04.5Test LossData Size BottleneckData Size21M43M86M172M344M688M1.4B22.0B104103102101NN/D/D0.00.10.20.30.40.5L/L(D=)1OverfittingData Size21M43M86M172M344M688M1.4B22.0B\\x0cFigure 10 The critical batch size Bcrit follows a power law in the loss as performance increase, and does\\nnot depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every\\n13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly\\npredicted by the gradient noise scale, as in [MKAT18].\\n\\ndeﬁning\\n\\nδL(N, D) ≡\\n\\nL(N, D)\\nL(N, ∞)\\n\\n− 1\\n\\n(4.2)\\n\\nand studying it as a function of N, D. In fact, we see empirically that δL depends only a speciﬁc combination\\nof N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies\\n\\n(cid:32)\\n\\nδL ≈\\n\\n1 +\\n\\n(cid:33)αD\\n\\n(cid:18) N\\nNc\\n\\n(cid:19) αN\\n\\nαD Dc\\nD\\n\\n− 1\\n\\n(4.3)\\n\\nNote that at large D this formula also has a series expansion in powers of 1/D.\\n\\nWe estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to\\navoid overﬁtting when training to within that threshold of convergence we require\\n\\nD (cid:38) (5 × 103) N 0.74\\n\\n(4.4)',\n",
       " 'With this relation, models smaller than 109 parameters can be trained with minimal overﬁtting on the 22B\\ntoken WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this\\nrelation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however\\nthat this does not typically represent maximally compute-efﬁcient training. We should also emphasize that\\nwe have not optimized regularization (eg the dropout probability) while varying dataset and model size.\\n\\n5 Scaling Laws with Model Size and Training Time\\n\\nIn this section we will demonstrate that a simple scaling law provides a good description for the loss as a\\nfunction of model size N and training time. First we will explain how to use the results of [MKAT18] to\\ndeﬁne a universal training step Smin, which accounts for the fact that most of our models have not been\\ntrained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time\\ndependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation\\nof training compute between model size and training time, and then conﬁrm that prediction.\\n\\n5.1 Adjustment for Training at Bcrit(L)\\n\\nA simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also\\n[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit\\nthe batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > Bcrit\\nincreases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple\\n\\n12',\n",
       " '1013×1004×1006×100WebText2 Train Loss103104105106Critical Batch Size (Tokens)Critical Batch Size vs. PerformanceEmpirical Bcrit, N=3MEmpirical Bcrit, N=85MBcrit=2.1×108tokensL4.8Noise Scale Measurement\\x0cprediction for Bcrit, and that neither depends directly on model size except through the value of the loss that\\nhas been attained. These results can be used to predict how training time and compute will vary with the\\nbatch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch\\nsize B ≈ Bcrit. Training at B (cid:29) Bcrit minimizes the number of training steps, while B (cid:28) Bcrit minimizes\\nthe use of compute.\\n\\nMore speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training\\nsteps S and the number of data examples processed E = BS satisfy the simple relation\\n\\n(cid:18) S\\n\\nSmin\\n\\n(cid:19) (cid:18) E\\n\\n− 1\\n\\nEmin\\n\\n(cid:19)\\n\\n− 1\\n\\n= 1\\n\\n(5.1)\\n\\nwhen training to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach\\nL, while Emin is the minimum number of data examples that must be processed.\\n\\nWe demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the\\ncritical batch size\\n\\nwhich is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal\\ntime/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.\\nIn Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for\\ntwo different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So\\nthe predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can\\nbe ﬁt with a power-law in the loss\\n\\nBcrit(L) ≡\\n\\nEmin\\nSmin\\n\\nBcrit(L) ≈\\n\\nB∗\\nL1/αB\\n\\n(5.2)\\n\\n(5.3)',\n",
       " 'where B∗ ≈ 2 × 108 and αB ≈ 0.21.\\nWe have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,\\nthe gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not\\nknow Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural\\nlanguage is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used\\na parameterization where Bcrit diverges as L → 0.\\n\\nWe will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch\\nsize B = 219 tokens and the number of training steps while training at B (cid:29) Bcrit. This is simply\\n\\nSmin(S) ≡\\n\\nS\\n1 + Bcrit(L)/B\\n\\n(minimum steps, at B (cid:29) Bcrit)\\n\\n(5.4)\\n\\nfor any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L\\nwith a model of size N if we were to train at B (cid:28) Bcrit(L). This is\\n\\nCmin(C) ≡\\n\\nC\\n1 + B/Bcrit(L)\\n\\n(minimum compute, at B (cid:28) Bcrit)\\n\\n(5.5)\\n\\nwhere C = 6N BS estimates the (non-embedding) compute used at batch size B.\\n\\n5.2 Results for L(N, Smin) and Performance with Model Size and Compute\\n\\nNow we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the\\nloss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training\\nruns using Equation (1.6), repeated here for convenience:\\n\\nL(N, Smin) =\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αS\\n\\n+\\n\\n(cid:18) Sc\\nSmin\\n\\n(5.6)\\n\\nfor the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt\\nto the data with the parameters:',\n",
       " '5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of\\n\\nBcrit from Figures 18 and 10 for all our later analyses.\\n\\n13\\n\\n\\x0cFigure 11 When we hold either total compute or number of training steps ﬁxed, performance follows\\nL(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that\\nmaximizes performance. Mediocre ﬁts at small S are unsurprising, as the power-law equation for the learning\\ncurves breaks down very early in training.\\n\\nParameter\\n\\nαN\\n\\nαS\\n\\nNc\\n\\nSc\\n\\nValue\\n\\n0.077\\n\\n0.76\\n\\n6.5 × 1013\\n\\n2.1 × 103\\n\\nTable 3 Fits to L(N, S)\\n\\nWith these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe\\nthey are quite compelling given the simplicity of Equation (5.6).\\n\\nThe data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we\\nstudy the test loss as a function of model size while ﬁxing either the total non-embedding compute C used\\nin training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters\\nabove and Equation (5.6).\\n\\nThe power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss\\nlandscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-\\nlaw should provide information about the spectrum of the Hessian of the loss. Its universality suggests that\\nthe Hessian eigenvalue density is roughly independent of model size.\\n\\n5.3 Lower Bound on Early Stopping Step',\n",
       " 'The results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which\\nearly stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D\\nlearning curves for a given model will be very similar until we reach Smin ≈ Sstop. Thus overﬁtting should\\nbe proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because\\nin reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more\\ntraining steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality\\n\\nSstop(N, D) (cid:38)\\n\\nSc\\n[L(N, D) − L(N, ∞)]1/αS\\n\\n(5.7)\\n\\nwhere L(N, ∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-\\nparison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop\\nand L(N, D) are empirical (though Sstop is adjusted to mimic training at B (cid:29) Bcrit), while L(N, ∞) is\\ncomputed from the ﬁt to L(N, D) evaluated at D = ∞.\\n\\n6 Optimal Allocation of the Compute Budget\\n\\nWe displayed the empirical trend of performance as a function of the computation used during training in\\nthe top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know\\n\\n14',\n",
       " '104106108Parameters (non-embedding)2345678Test LossPerformance vs Compute Budget105104103102101100PF-dayss106107108109Parameters (non-embedding)2.43.03.64.24.85.4Test LossPerformance vs Steps104105Steps\\x0cFigure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger\\nor smaller models can be trained with minimal additional compute. Right: Models larger than the compute-\\nefﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-\\nlelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the\\npower-law region of the learning curve, after initial transient effects.\\n\\nFigure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a\\nsomewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous\\nlump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks\\nin the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger\\ncompute.\\n\\nthat in fact we could train more efﬁciently6 by training at the batch size Bcrit discussed in Section 5.1.\\nLarge and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,\\nand correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more\\npredictable trends.\\n\\nIn this section we will adjust for this oversight. More importantly, we will use the results of Section 5\\nto determine the optimal allocation of compute between model size N and the quantity of data processed\\nduring training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by\\nusing the equation for L(N, Smin), and we will demonstrate that these methods agree.\\n\\n6.1 Optimal Performance and Allocations\\n\\nLet us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is\\nplotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the\\nnew ﬁt with Cmin is somewhat improved.',\n",
       " 'Given L(Cmin), it is natural to ask for the optimal model size N (Cmin) that provides the minimal loss with a\\ngiven quantity of training compute. The optimal model size is shown in Figure 14. We observe that N (Cmin)\\n\\n6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the\\n\\nmodel but also on the target value of the loss we wish to achieve, and so is a moving target.\\n\\n15\\n\\nModels between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budgetSmaller models require more steps to train, while larger models require fewerOur framework does not capture early training dynamics108106104102100Compute (PF-days), non-embedding234567Test LossL=(Cmin/2.3108)0.050L=(C/2.0107)0.057\\x0cFigure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N . Optimal\\nmodel size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number\\nof data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.\\nRight: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most\\nof the growth in data examples processed can be used for increased batch sizes.\\n\\ncan be ﬁt very well with a power-law\\n\\nN (Cmin) ∝ (Cmin)0.73.\\n\\n(6.1)\\n\\nIn Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).\\n\\nBy deﬁnition Cmin ≡ 6N BcritS, and so we can use N (Cmin) to extract further results. In particular, since\\nprior ﬁts show B ∝ L−4.8 and L ∝ C −0.05\\nmin . This leads us to conclude\\nthat the optimal number of steps will only grow very slowly with compute, as\\n\\nmin , we can conclude that Bcrit ∝ C 0.24\\n\\nSmin ∝ (Cmin)0.03,\\n\\n(6.2)',\n",
       " 'matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results\\nmay even be consistent with an exponent of zero.\\n\\nThus we conclude that as we scale up language modeling with an optimal allocation of computation, we\\nshould predominantly increase the model size N , while simultaneously scaling up the batch size via B ∝\\nBcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively\\nfew optimization steps, additional work on speeding up early training dynamics may be warranted.\\n\\n6.2 Predictions from L(N, Smin)\\n\\nThe results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in\\nSection 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin\\n6N B and then ﬁnd the minimum\\nof the loss as a function of N , while ﬁxing the training compute. We carry out this procedure in detail in\\nAppendix B, where we also provide some additional predictions.\\n\\nFor the loss as a function of training compute, we predict that\\n\\nwhere\\n\\nL(Cmin) =\\n\\n(cid:19)αmin\\n\\nC\\n\\n(cid:18) C min\\nc\\nCmin\\n\\nαmin\\n\\nC ≡\\n\\n1\\n1/αS + 1/αB + 1/αN\\n\\n≈ 0.054\\n\\nin excellent agreement with the exponent of Figure 13. We also predict that\\n\\nN (Cmin) ∝ (Cmin)αmin\\n\\nC /αN ≈ (Cmin)0.71\\n\\n(6.3)\\n\\n(6.4)\\n\\n(6.5)\\n\\nwhich also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive\\nframework for the performance of language modeling.\\n\\n16',\n",
       " '107105103101Compute (PF-days), non-embedding103105107Parameters (non-embedding)N=(1.3109)C0.73minN=(1.6109)C0.88107105103101Compute (PF-days), excluding embeddings050001000015000StepsSmin (adjusted)Smin=(5.4103)C0.03minS (fixed-batch)\\x0cFigure 15 Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations\\nfor L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection\\nmarks the point before which we expect our predictions to break down. The location of this point is highly\\nsensitive to the precise exponents from our power-law ﬁts.\\n\\n6.3 Contradictions and a Conjecture\\n\\nWe observe no signs of deviation from straight power-law trends at large values of compute, data, or model\\nsize. Our trends must eventually level off, though, since natural language has non-zero entropy.\\n\\nIndeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-\\ndiction. At scales several orders of magnitude above those documented here, the performance predicted by\\nthe L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with\\ncompute. This implies that our scaling laws must break down before this point, but we conjecture that the\\nintersection point has a deeper meaning: it provides an estimate of the point at which Transformer language\\nmodels reach maximal performance.\\n\\nSince the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the\\nperformance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).\\nLet us work this out in more detail.\\n\\nTo keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as\\n\\nD ∝ N 0.74 ∝ C 0.54\\nmin\\n\\n(6.6)\\n\\nwhere we have used the compute-efﬁcient N (Cmin) from Figure 14.',\n",
       " 'Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch\\nsize (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as\\n\\nD(Cmin) =\\n\\n2Cmin\\n6N (Cmin)\\n\\n≈ (cid:0)4 × 1010 tokens(cid:1) (Cmin/PF-Day)0.26\\n\\n(6.7)\\n\\nThis is the maximum rate at which the dataset size can productively grow with compute, since it means that\\nwe are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).\\nIt appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if\\nthe training process never re-uses any data!\\n\\nAccording to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the\\nloss should scale as L(D) ∝ D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝\\nC −0.03\\nonce we are data-limited. Once again, we have a contradiction, as this will eventually intersect with\\nmin\\nour prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝ C −0.050\\nThe intersection point of L(D(Cmin)) and L(Cmin) occurs at\\n\\nmin\\n\\n.\\n\\nC ∗ ∼ 104 PF-Days N ∗ ∼ 1012 parameters, D∗ ∼ 1012 tokens, L∗ ∼ 1.7 nats/token\\n\\n(6.8)\\n\\nthough the numerical values are highly uncertain, varying by an order or magnitude in either direction de-\\npending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is\\nthat our scaling laws break down at or before we reach this point, which is still many orders of magnitude\\naway in both compute and model size.\\n\\n17',\n",
       " 'The intersection point is sensitive to the precise power-law parameters\\x0cOne might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\\nsize beyond N ∗ without qualitatively different data requirements, perhaps this means that once we reach\\nC ∗\\nmin and N ∗, we have extracted all of the reliable information available in natural language data. In this\\ninterpretation, L∗ would provide a rough estimate for the entropy-per-token7 of natural language. In this\\nscenario, we would expect the loss trend to level off at or before L∗.\\n\\nWe can guess at the functional form of L(Cmin) as it levels off by considering a version of our training\\ndataset with added noise. For example, we could append a random string of tokens to each context shown\\nto the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise\\nﬂoor L − Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\\npotentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect\\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and\\nmay be meaningful even if it occurs after the leveling off.\\n\\n7 Related Work\\n\\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\\nnumber of relevant features in the data.',\n",
       " 'Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More\\nrecent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is\\nperhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of\\ndataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our\\nﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets\\n[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent\\nwork [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an\\nansatz similar to ours.\\n\\nEfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal\\nperformance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that\\nfor language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).\\nBut more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the\\noverall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles\\nof shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width\\nand depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies\\nﬁx computation per data example, which tends to scale in proportion to the number of model parameters,\\nwhereas we investigate scaling with both model size and the quantity of training computation.',\n",
       " 'Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-\\ning a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training\\nmany orders of magnitude beyond typical practice, and in particular does not use early stopping). We do\\nnot observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.\\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\\nHessian spectrum [Pap18, GKX19, GARD18].\\n\\n8 Discussion\\n\\nWe have observed consistent scalings of language model log-likelihood loss with non-embedding parameter\\ncount N , dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and\\n(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.\\nSince scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale.\\n\\n7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.\\n8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of\\n\\nloss on both model and dataset size.\\n\\n18\\n\\n\\x0cWe were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when\\nthese parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\\nof overﬁtting, early stopping step, and data requirements when training large language models. So our scaling\\nrelations go beyond mere observation to provide a predictive framework. One might interpret these relations\\nas analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,\\nindependent of most of the details of its microscopic consituents.',\n",
       " 'It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a\\nmaximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to\\ntest these relations on other domains, such as images, audio, and video models, and perhaps also for random\\nnetwork distillation. At this point we do not know which of our results depend on the structure of natural\\nlanguage data, and which are universal.\\nIt would also be exciting to ﬁnd a theoretical framework from\\nwhich the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we\\nhave observed. Such a theory might make it possible to derive other more precise predictions, and provide a\\nsystematic understanding of the limitations of the scaling laws.\\n\\nIn the domain of natural language, it will be important to investigate whether continued improvement on the\\nloss translates into improvement on relevant language tasks. Smooth quantitative change can mask major\\nqualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy\\nprovides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth\\nimprovements in language model loss may hide seemingly qualitative changes in capability.\\n\\nOur results strongly suggest that larger models will continue to perform better, and will also be much more\\nsample efﬁcient than has been previously appreciated. Big models may be more important than big data.\\nIn this context, further investigation into model parallelism is warranted. Deep models can be trained using\\npipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased\\nbatch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization\\n[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity\\n[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through\\nincreased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,\\nit might be possible to remain on the compute-efﬁcient frontier for an entire training run.\\n\\nAcknowledgements',\n",
       " 'We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\\nback on drafts of this work.\\n\\n19\\n\\n\\x0cAppendices\\n\\nA Summary of Power Laws\\n\\nFor easier reference, we provide a summary below of the key trends described throughout the paper.\\n\\nParameters Data Compute\\n\\nBatch Size Equation\\n\\nN\\n\\n∞\\n\\nOptimal\\n\\n∞\\n\\nD\\n\\n∞\\n\\n∞\\n\\nEarly Stop\\n\\nC\\n\\nFixed\\n\\nFixed\\n\\nFixed\\n\\nL (N ) = (Nc/N )αN\\nL (D) = (Dc/D)αD\\nL (C) = (Cc/C)αC (naive)\\n\\nNopt\\n\\nDopt\\n\\nCmin\\n\\nN\\n\\nN\\n\\nD\\n\\n∞\\n\\nEarly Stop\\n\\nB (cid:28) Bcrit L (Cmin) = (cid:0)C min\\nc\\n(cid:0) Nc\\nN\\n\\nL (N, D) =\\n\\nFixed\\n\\n(cid:20)\\n\\n(cid:1)αmin\\n\\nC\\n\\n(cid:21)αD\\n\\n/Cmin\\n(cid:1) αN\\nαD + Dc\\nD\\n(cid:16)\\n\\n(cid:1)αN +\\n\\n(cid:17)αS\\n\\nSc\\nSmin(S,B)\\n\\nS steps\\n\\nB\\n\\nL (N, S) = (cid:0) Nc\\n\\nN\\n\\nTable 4\\n\\nThe empirical ﬁtted values for these trends are:\\n\\nPower Law\\n\\nScale (tokenization-dependent)\\n\\nαN = 0.076\\n\\nαD = 0.095\\n\\nNc = 8.8 × 1013 params (non-embed)\\nDc = 5.4 × 1013 tokens\\nCc = 1.6 × 107 PF-days\\n\\nαC = 0.057\\nC = 0.050 C min\\nαmin\\nαB = 0.21\\n\\nαS = 0.76\\n\\nc = 3.1 × 108 PF-days\\n\\nB∗ = 2.1 × 108 tokens\\nSc = 2.1 × 103 steps\\n\\nThe optimal parameters for compute efﬁcient training are given by:\\n\\nTable 5\\n\\nCompute-Efﬁcient Value\\n\\nPower Law Scale',\n",
       " 'Nopt = Ne · C pN\\nmin\\n= BeC pB\\nB (cid:28) Bcrit = B∗\\nmin\\nL1/αB\\nSmin = Se · C pS\\nmin (lower bound)\\nDopt = De · C pD\\nmin (1 epoch)\\n\\npN = 0.73\\n\\npB = 0.24\\n\\npS = 0.03\\n\\npD = 0.27\\n\\nNe = 1.3 · 109 params\\nBe = 2.0 · 106 tokens\\nSe = 5.4 · 103 steps\\nDe = 2 · 1010 tokens\\n\\nTable 6\\n\\nB Empirical Model of Compute-Efﬁcient Frontier\\n\\nThroughout this appendix all values of C, S, and αC are adjusted for training at the critical batch size Bcrit.\\nWe have left off the ‘adj’ label to avoid cluttering the notation.\\n\\nB.1 Deﬁning Equations\\n\\nThe power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this\\nappendix, we will derive the optimal performance, model size, and number of training steps as a function of\\n\\n20\\n\\n\\x0cthe compute budget. We start with the Equation (1.6), repeated here for convenience:\\n\\nL (N, S) =\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n+\\n\\n(cid:18) Sc\\nS\\n\\n(cid:19)αS\\n\\n.\\n\\n(B.1)\\n\\nHere, S represents the number of parameter updates when training at the critical batch size [MKAT18],\\nwhich was deﬁned in Equation (5.2)9:\\n\\nB (L) =\\n\\nB∗\\nL1/αB\\n\\n.\\n\\n(B.2)\\n\\nWe would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =\\nC/ (6N B (L)), where C is the number of FLOPs used in the training run:\\n\\nL (N, C) =\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN\\n\\n(cid:18)\\n\\n+\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:19)αS\\n\\n.\\n\\nNow, we set ∂N L(cid:12)\\n\\n(cid:12)C = 0 to ﬁnd the condition for optimality:\\n\\n0 =',\n",
       " '∂L\\n∂N\\n\\n(cid:12)\\n(cid:12)C\\nαN\\nN\\n\\n= −\\n\\n(cid:19)αN\\n\\n(cid:18) Nc\\nN\\n\\n+\\n\\nαS\\nN\\n(cid:19)αS\\n\\n=⇒\\n\\nαN\\nαS\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN\\n\\n(cid:18)\\n\\n=\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:18)\\n\\n6B∗Sc\\n\\nN\\nL1/αB C\\n\\n(cid:19)αS (cid:18)\\n\\n1 − 5\\n\\n(cid:19)\\n\\nN\\nL (cid:26)\\n\\n(cid:26)∂L\\n(cid:12)\\n(cid:26)\\n(cid:12)C\\n∂N\\n\\nEquation (B.3) and (B.4) together determine the compute-efﬁcient frontier.\\n\\nB.2 Efﬁcient Training\\n\\nNow we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields\\n\\nL (Neﬀ (C) , C) =\\n\\n1 +\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nαN\\nαS\\n\\nL (Neﬀ , ∞) ,\\n\\n(B.3)\\n\\n(B.4)\\n\\n(B.5)\\n\\nwhich implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN\\n≈ 10% above\\nαS\\nthe converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating\\nN yields a power-law dependence of performance on compute:\\n\\nwhere we deﬁned\\n\\nL (C) =\\n\\n(cid:19)αC\\n\\n(cid:18) Cc\\nC\\n\\nαC = 1/ (1/αS + 1/αB + 1/αN ) ≈ 0.052\\n\\nCc = 6NcB∗Sc\\n\\n1 +\\n\\n(cid:18)\\n\\nαN\\nαS\\n\\n(cid:19)1/αS +1/αN (cid:18) αS\\nαN\\n\\n(cid:19)1/αS\\n\\n.\\n\\nSimilarly, we can eliminate L to ﬁnd N (C):\\n\\nN (C)\\nNc\\n\\n=\\n\\n(cid:18) C\\nCc',\n",
       " '(cid:19)αC /αN (cid:18)\\n\\n(cid:19)1/αN\\n\\n1 +\\n\\nαN\\nαS\\n\\nand\\n\\n(B.6)\\n\\n(B.7)\\n\\n(B.8)\\n\\n(B.9)\\n\\n(cid:18)\\n\\nS (C) =\\n\\nαN\\nαS\\n9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could\\ninstead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is\\nthe averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see\\n[MKAT18]).\\n\\nCc\\n6NcB∗\\n\\n(B.10)\\n\\n1 +\\n\\n(cid:19)−1/αN (cid:18) C\\nCc\\n\\n(cid:19)αC /αS\\n\\n21\\n\\n\\x0cB.3 Comparison to Inefﬁcient\\n\\nTypically, researchers train models until they appear to be close to convergence. In this section, we compare\\nthe efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor\\nf as the percent deviation from the converged loss:\\n\\nL (N, C) = (1 + f ) L (N, ∞) .\\n\\n(B.11)\\n\\nFor compute-efﬁcient training we have f = αN /αS ≈ 10% from the previous section, but researchers\\ntypically use a much smaller value. Here, we choose f (cid:48) = 2% as an estimate. For a ﬁxed value of the loss,\\nwe predict:\\n\\nNf\\nNf (cid:48)\\n\\nSf\\nSf (cid:48)\\n\\nCf\\nCf (cid:48)\\n\\n=\\n\\n=\\n\\n=\\n\\n(cid:18) 1 + f\\n1 + f (cid:48)\\n(cid:32) 1 + 1\\nf\\n1 + 1\\nf (cid:48)\\nSf\\nSf (cid:48)\\n\\nNf\\nNf (cid:48)\\n\\n(cid:19)1/αN\\n\\n≈ 2.7\\n\\n(cid:33)1/αS\\n\\n≈ 0.13\\n\\n≈ 0.35\\n\\n(B.12)\\n\\n(B.13)\\n\\n(B.14)',\n",
       " 'So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less\\ncompute to reach the same loss.\\n\\nB.4 Suboptimal Model Sizes\\n\\nWe can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss\\nL with a model of size N :\\n\\n(cid:18)\\n\\nC (N, L) =\\n\\n6B∗Sc\\n\\nN\\nL1/αB\\n\\n(cid:19) (cid:18)\\n\\nL −\\n\\n(cid:18) Nc\\nN\\n\\n(cid:19)αN (cid:19)−1/αS\\n\\n.\\n\\n(B.15)\\n\\nUsing A.6 and A.9, we can eliminate L in favor of Neﬀ (L), the model size which reaches L most efﬁciently.\\nFrom there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal\\nmodel size:\\n\\n(cid:20)\\n\\n(cid:18)\\n\\n(cid:19)αN (cid:19)(cid:21)−1/αS\\n\\nC (N, Neﬀ )\\nC (Neﬀ , Neﬀ )\\n\\nαS\\nαN\\nThe result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a\\n20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A\\nlarger model can be trained the the same level of performance in fewer steps, allowing for more parallelism\\nand faster training if sufﬁcient harware is available (see Figure Y):\\n\\nN\\nNeﬀ\\n\\n(B.16)\\n\\n1 −\\n\\n1 +\\n\\n=\\n\\n.\\n\\n(cid:18) Neﬀ\\nN\\n\\nS (N, Neﬀ )\\nS (Neﬀ , Neﬀ )\\n\\n(cid:20)\\n\\n=\\n\\n1 +\\n\\nαS\\nαN\\n\\n(cid:18)\\n\\n1 −\\n\\n(cid:18) Neﬀ\\nN\\n\\n(cid:19)αN (cid:19)(cid:21)−1/αS\\n\\n.\\n\\n(B.17)',\n",
       " 'A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation\\nshould not be trusted for very large models, as it is only valid in the power-law region of the learning curve\\nafter initial transient effects.\\n\\nC Caveats\\n\\nIn this section we list some potential caveats to our analysis.\\n\\n• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.\\nThe scaling relations with model size and compute are especially mysterious. It may be possible to\\nunderstand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning\\ncurves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very\\nlarge model size still remains mysterious. Without a theory or a systematic understanding of the\\ncorrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.\\n\\n22\\n\\n\\x0cFigure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of\\noverﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:\\nWe display train and test loss for a series of 300M parameter models trained on different sized dataset sub-\\nsamples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the\\ndegree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest − Ltrain\\n(denoted by a black bar for each run).\\n\\n• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the\\nrange we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between\\ndata parallelism and the number of serial training steps required, which would have a major impact\\non training time.\\n\\n• We did not thoroughly investigate the small data regime, and our ﬁts for L(N, D) were poor for\\nthe smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did\\nnot experiment with regularization and data augmentation. Improvements in these could alter our\\nresults, quantitatively or qualitatively.',\n",
       " '• We used the estimated training compute C ≈ 6N BS, which did not include contributions propor-\\ntional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the\\nregime of very large nctx, speciﬁcally where nctx (cid:38) 12dmodel.\\n\\n• We tuned learning rates, and we experimented with learning rate schedules. But we may have\\nneglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important\\neffect on scaling.\\n\\n• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,\\nit may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short\\ntraining run (eg due to compute limitations), it may be possible to use a larger learning rate. We did\\nnot experiment with higher learning rates for training runs that did not proceed to convergence.\\n\\nD Supplemental Figures\\n\\nD.1 Early Stopping and Test vs Train\\n\\nIn section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on\\nthe early stopping step. We also show the train and test loss for a given model size when training on different\\nsized datasets.\\n\\nD.2 Universal Transformers\\n\\nWe compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.\\nThese models re-use parameters, and so perform slightly better as a function of N , but slightly worse as a\\nfunction of compute C. We include several different different possibilities for parameter re-use.\\n\\nD.3 Batch Size\\n\\nWe measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate\\nBcrit(L) in ﬁgure 10.\\n\\n23\\n\\n103104105Sc×[L(N,D)L(N,)]1/S103104105SstopEarly Stopping StepData Size21M43M86M172M344M688M1.4B103104105Step23456LossTest LossTrain Loss1081091010Dataset Size (Tokens)\\x0cFigure 17 We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-\\nformers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,\\nbut slightly worse when accounting for reuse and comparing per FLOP.',\n",
       " 'Figure 18 These ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and\\nfor two different Transformer model sizes. These ﬁts were used to measure Bcrit(L) for Figure 10.\\n\\nD.4 Sample Efﬁciency vs Model Size\\n\\nIt is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We\\nprovide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach\\nvarious ﬁxed values of the loss.\\n\\nFigure 19 The number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases\\nprecipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)\\nimproves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model\\nto a very large one.\\n\\n24\\n\\n105106107108109Parameters, including reuse (non-embedding)2.53.03.54.04.5Test Loss2x Reuse4x Reuse8x ReuseNon-recurrent Models105106107108109Parameters (non-embedding)2.53.03.54.04.5Test Loss2x Reuse4x Reuse8x ReuseNon-recurrent Models102103104105Step10610710810910101011Tokens ProcessedBatch Size Scan - 3M Params46810Test Loss101102103104105Step1061081010Tokens ProcessedBatch Size Scan - 85M Params46810Test Loss106107108Parameters (non-embedding)103104105Minimum Steps (Smin)2.53.03.54.04.55.05.5Loss106107108Parameters (non-embedding)10810910101011Minimum Examples (Emin)2.53.03.54.04.55.05.5Loss\\x0cFigure 20 This ﬁgure provides information about the performance per token as a function of model size\\nand training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales\\npredictably as a power-law in T . Right: Test loss per token as a function of training step.',\n",
       " 'Figure 21\\nIn addition to the averaged loss, individual tokens within the 1024-token context also improve\\nsmoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better\\non early tokens, since they can allocate all of their capacity to them.\\n\\nD.5 Context Dependence\\n\\nThe trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.\\nWe see that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst\\ntoken.\\n\\nFixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see\\nFigure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,\\nLT16], or a more general feature of the model architecture and optimization. It provides some suggestion for\\nthe potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge\\nto better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger\\nmodels are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we\\nshow how per-token performance varies for a ﬁxed model as a function of the training step. The model begins\\nby learning short-range information, and only learns longer-range correlations later in training.\\n\\nWe have also included models trained with a tiny context nctx = 8 in order to compare with our longer\\ncontext models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024\\nmodels on very early tokens. This also suggests that further improvements should be possible with much\\nlarger models trained on large contexts.\\n\\nD.6 Learning Rate Schedules and Error Analysis\\n\\nWe experimented with a variety of learning rates and schedules. A host of schedules and resulting test\\nperformances for a small language model are plotted in Figure 22. We conclude that the choice of learning\\nrate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the\\nschedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among\\n\\n25',\n",
       " '100101102103Token Index345678Per-Token Test Loss4.0+3.2T0.473.4+4.0T0.562.9+4.5T0.562.7+4.9T0.602.4+5.1T0.612.3+5.4T0.62106107108Model Parameters101103105Step246810Test LossPer-token Loss (774M Params)100101102103Token Index104105106107108109Parameters (excl. embedding)3.04.56.07.5Test LossToken 1/1024Token 2/1024Token 4/1024Token 8/1024Token 16/1024Token 64/1024Token 256/1024Token 1024/1024Token 1/8Token 2/8Token 4/8Token 8/8\\x0cFigure 22 We test a variety of learning rate schedules including cosine decay, linear decay, as well as other\\nfaster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we\\ndo not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.\\nWe ﬁnd that, as long as the learning rate is not too small and does not decay too quickly, performance does\\nnot depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging\\nmultiple runs is necessary to validate performance changes smaller than this level.\\n\\nFigure 23 The trend for performance as a function of parameter count, L(N ), is ﬁt better by a power law\\nthan by other functions such as a logarithm at a qualitative level.\\n\\nschedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different\\ntraining runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different\\nrandom seeds is roughly constant in magnitude for different model sizes.\\n\\nWe found that larger models require a smaller learning rate to prevent divergence, while smaller models can\\ntolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:\\n\\nLR(N ) ≈ 0.003239 + −0.0001395 log(N )\\n\\n(D.1)',\n",
       " 'We expect that this formula could be improved. There may be a dependence on network width, likely set by\\nthe initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that\\nit works sufﬁciently well for the models we considered.\\n\\nD.7 Fit Details and Power Law Quality\\n\\nWe experimented with a number of functional forms for the ﬁts to L(N ), L(C), and L(D); the power-law\\nﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).\\n\\nFor L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers\\ncauses a noticable lump in the data. For L(N ) we also do not include very small models with only 1 layer in\\nthe ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change\\nmarginally if we do include them, and the trend extrapolates well in both directions regardless.\\n\\nD.8 Generalization and Architecture\\n\\nIn ﬁgure 24 we show that generalization to other data distributions does not depend on network depth when we\\nhold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.\\n\\n26\\n\\n050000100000150000200000250000Step0.00000.00020.00040.00060.00080.0010Learning Rate50100150200250LR Summed Over Steps3.653.703.753.803.853.90Loss104105106107108109Parameters (non-embedding)23456Test Loss (at convergence)L=(N/8.81013)0.076L=0.25log(N/7.11012)\\x0cFigure 24 We show evaluations on a series of datasets for models with approximately 1.5 Billion param-\\neters. We observe no effect of depth on generalization; generalization performance depends primarily on\\ntraining distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the\\nearly-stopped performance; we have not seen this surprising result in other experiments.\\n\\nList of Figures\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nSummary of simple power laws.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " 'Illustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .\\n\\nHow to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . .\\n\\nPerformance when varying model and data size, or model and training steps, simultaneously\\n\\n5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . .\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nComparison of performance trend when including or excluding embeddings . . . . . . . . .\\n\\nLSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . .\\n\\nGeneralization to other test datasets\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nUniversality of overﬁtting .\\n\\n10 Critical batch size .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n11\\n\\n12\\n\\nPerformance versus compute budget or number of parameter updates . . . . . . . . . . . . .\\n\\nTraining on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n13 Comparison between empirical and adjusted compute trends\\n\\n. . . . . . . . . . . . . . . . .\\n\\n14 Optimal model size and serial number of steps versus compute budget\\n\\n. . . . . . . . . . . .\\n\\n15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n16\\n\\nEarly stopping lower bound and training curves for overﬁt models\\n\\n. . . . . . . . . . . . . .\\n\\n17 Universal transformers\\n\\n18 Batch size scans .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n19 Another look at sample efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n20\\n\\n21\\n\\n22\\n\\nPower-law dependence of performance on position in context . . . . . . . . . . . . . . . . .\\n\\nPerformance at different context positions versus model size . . . . . . . . . . . . . . . . .\\n\\nLearning rate schedule scan .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n23 Comparison of Power-Law and Logarithmic Fits\\n\\n. . . . . . . . . . . . . . . . . . . . . . .\\n\\n24 Generalization versus depth .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n27\\n\\n3\\n\\n4\\n\\n4\\n\\n5\\n\\n8\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n12\\n\\n14\\n\\n15\\n\\n15\\n\\n16\\n\\n17\\n\\n23\\n\\n24\\n\\n24\\n\\n24\\n\\n25\\n\\n25\\n\\n26\\n\\n26\\n\\n27\\n\\n101102Depth2.32.42.52.62.72.8Test LossWikipediaBooksInternet BooksCommon CrawlWebText2 (Train)WebText2 (Test)\\x0cList of Tables\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nParameter and compute counts for Transformer\\n\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n\\nFits to L(N, D) .\\n\\nFits to L(N, S) .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\nKey trend equations .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . .\\n\\n. . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . .\\n\\n. . .',\n",
       " '. . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nKey parameters to trend ﬁts .\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nTrends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n7\\n\\n11\\n\\n14\\n\\n20\\n\\n20\\n\\n20\\n\\nReferences\\n\\n[ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-\\nrange correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–\\n11587, 2012. 25\\n\\n[AS17]\\n\\n[BB01]\\n\\nMadhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in\\nneural networks. arXiv, 2017, 1710.03667. 11, 18, 22\\n\\nMichele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-\\nbiguation. In Proceedings of the 39th annual meeting on association for computational linguis-\\ntics, pages 26–33. Association for Computational Linguistics, 2001. 18\\n\\n[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine\\n\\nlearning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18\\n\\n[Bia12]\\n\\nGÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,\\n13(Apr):1063–1095, 2012. 18\\n\\n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/\\nabs/1904.10509. 19',\n",
       " '[DCLT18]\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2\\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-\\nversal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/\\nabs/1807.03819. 6, 9, 23, 24\\n\\n[EP94]\\n\\nWerner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.\\nEPL (Europhysics Letters), 26(4):241, 1994. 25\\n\\n[Fou]\\n\\nThe Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7\\n\\n[GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.\\n\\n2018, arXiv:1812.04754. 18\\n\\n[GJS+19] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,\\nGiulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with\\nnumber of parameters in deep learning. arXiv, 2019, 1901.01608. 18\\n\\n[GKX19]\\n\\n[Goo01]\\n\\n[GRK17]\\n\\n[HAD19]\\n\\nBehrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-\\ntimization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL\\nhttp://arxiv.org/abs/1901.10159. 18\\n\\nJoshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL\\nhttp://arxiv.org/abs/cs.CL/0108005. 18',\n",
       " 'Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-\\nnai.com, 2017. 19\\n\\nJoel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-\\ntational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and\\nPractice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.\\ndoi:10.1145/3293883.3295710. 18\\n\\n28\\n\\n\\x0c[HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\\nand Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\\nCoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19\\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\\nninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\\ndictable, empirically, 2017, 1712.00409. 18\\n\\n[JGH18]\\n\\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\\ngeneralization in neural networks. In Advances in neural information processing systems, pages\\n8571–8580, 2018. 18\\n\\n[KB14]\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014,\\n1412.6980. 7\\n\\n[Kom19]\\n\\nAran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18\\n\\n[KSH12]',\n",
       " 'Imagenet classiﬁcation with deep\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\\nInformation Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran\\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19\\n\\n[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\\nSoricut. Albert: A lite bert for self-supervised learning of language representations, 2019,\\n1909.11942. 9\\n\\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-\\ning approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/\\n1907.11692. 2\\n\\n[LSP+18]\\n\\n[LT16]\\n\\n[LXS+19]\\n\\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],\\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6\\nHenry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv\\npreprint arXiv:1606.06737, 2016. 25\\n\\nJaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\\nunder gradient descent, 2019, arXiv:1902.06720. 18',\n",
       " '[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\\n\\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\\n\\n[Pap18]\\n\\nVardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\\nCoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18\\n\\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\\n\\nImproving language\\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\\nassets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6\\n\\n[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\n\\nprediction of the generalization error across scales, 2019, 1909.12673. 18\\n\\n[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\nprediction of the generalization error across scales, 2019, arXiv:1909.12673. 18\\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer, 2019, arXiv:1910.10683. 2\\n\\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\n\\nmodels are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8',\n",
       " '[SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\\nBlake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19\\n\\n[SHB15]\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. CoRR, 2015, 1508.07909. 6\\n\\n29\\n\\n\\x0c[SS18]\\n\\n[SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,\\narXiv:1811.03600. 12\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\\ncost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.\\n7\\nStefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.\\nOxford University Press, 2018. 18\\nMingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\\nnetworks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.\\n11946. 18\\n\\n[THK18]\\n\\n[TL19]',\n",
       " '[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6\\n\\n[VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles\\n\\n[Was06]\\n\\nof relatively shallow networks, 2016, arXiv:1605.06431. 8, 18\\nLarry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.\\n18\\n\\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\\nOmer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose\\nlanguage understanding systems, 2019, 1905.00537. 2\\n\\n[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-\\ncreasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19\\n\\n[WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional\\n\\nnetworks, 2019, 1906.02909. 19\\n\\n[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\\nXlnet: Generalized autoregressive pretraining for language understanding, 2019,',\n",
       " 'Le.\\narXiv:1906.08237. 2\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British\\nMachine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18\\n\\n[ZK16]\\n\\n[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\\nwatching movies and reading books. 2015 IEEE International Conference on Computer Vision\\n(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7\\n\\n[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,\\nChristopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch\\nsizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL\\nhttp://arxiv.org/abs/1907.04164. 12, 18\\n\\n30']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 274M/274M [00:11<00:00, 23.2MiB/s] \n",
      "Verifying: 100%|██████████| 274M/274M [00:00<00:00, 542MiB/s] \n"
     ]
    }
   ],
   "source": [
    "from nomic import embed\n",
    "\n",
    "embed_res = embed.text(\n",
    "    texts=chunks,\n",
    "    model=\"nomic-embed-text-v1.5\",\n",
    "    task_type=\"search_document\",\n",
    "    inference_mode=\"local\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 53 vector embeddings, 24463 total tokens\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\\nCreated {len(embed_res['embeddings'])} vector embeddings, \"\n",
    "    f\"{embed_res['usage']['total_tokens']} total tokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(query_vector, vectors):\n",
    "    query_vector = np.array(query_vector)\n",
    "    vectors = np.array(vectors)\n",
    "    return np.dot(vectors, query_vector) / (\n",
    "        np.linalg.norm(vectors, axis=1) * np.linalg.norm(query_vector)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embed_res.get(\"embeddings\")[0]\n",
    "vectors = np.array(embed_res.get(\"embeddings\")[1:])\n",
    "value, index = cosine_similarity(query_vector, vectors).max(0), cosine_similarity(\n",
    "    query_vector, vectors\n",
    ").argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9048170795401734), np.int64(0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "VECTOR_STORE_FILEPATH = \"data/vector_store.json\"\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.store = []\n",
    "\n",
    "    def add(self, items):\n",
    "        self.store.extend(items)\n",
    "\n",
    "    def save(self, file_path=VECTOR_STORE_FILEPATH):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(self.store, f)\n",
    "\n",
    "    def load(self, file_path=VECTOR_STORE_FILEPATH):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.store = json.load(f)\n",
    "\n",
    "    def query(self, vector, top_k=10):\n",
    "        vectors = [item[\"vector\"] for item in self.store]\n",
    "        similarities = cosine_similarity(vector, vectors)\n",
    "        top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [{**self.store[i], \"score\": similarities[i]} for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vectors = [\n",
    "    {\"vector\": vector, \"text\": text}\n",
    "    for vector, text in zip(embed_res[\"embeddings\"], chunks)\n",
    "]\n",
    "vector_store.add(vectors)\n",
    "vector_store.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vector': [0.025192266330122948,\n",
       "  0.10301493853330612,\n",
       "  -0.16977433860301971,\n",
       "  -0.054056257009506226,\n",
       "  0.0472666397690773,\n",
       "  0.00872587040066719,\n",
       "  0.02851969748735428,\n",
       "  -0.0030272614676505327,\n",
       "  -0.058842822909355164,\n",
       "  0.04434778913855553,\n",
       "  -0.04581887647509575,\n",
       "  0.008022510446608067,\n",
       "  0.09275990724563599,\n",
       "  0.027995722368359566,\n",
       "  -0.006200325675308704,\n",
       "  0.023318292573094368,\n",
       "  -0.0038849851116538048,\n",
       "  -0.0360867865383625,\n",
       "  -0.02529829554259777,\n",
       "  0.006476488430052996,\n",
       "  -0.04451005905866623,\n",
       "  -0.012459545396268368,\n",
       "  0.014795361086726189,\n",
       "  -0.05941741168498993,\n",
       "  0.03610574081540108,\n",
       "  0.05737169459462166,\n",
       "  -0.024454595521092415,\n",
       "  -0.026275992393493652,\n",
       "  -0.04933438077569008,\n",
       "  -0.002469467930495739,\n",
       "  0.059806209057569504,\n",
       "  -0.026146337389945984,\n",
       "  -0.023411475121974945,\n",
       "  -0.07631231099367142,\n",
       "  -0.09153863042593002,\n",
       "  -0.006486288271844387,\n",
       "  0.04220665991306305,\n",
       "  -0.0010079490020871162,\n",
       "  0.028497548773884773,\n",
       "  0.04629940539598465,\n",
       "  0.04328909516334534,\n",
       "  -0.008387215435504913,\n",
       "  -0.028204547241330147,\n",
       "  -0.03494105115532875,\n",
       "  0.047928038984537125,\n",
       "  -0.025806525722146034,\n",
       "  0.01831531524658203,\n",
       "  -0.00959481205791235,\n",
       "  0.055988654494285583,\n",
       "  -0.04725048691034317,\n",
       "  0.013388093560934067,\n",
       "  0.02351991832256317,\n",
       "  -0.015401131473481655,\n",
       "  -0.011592891067266464,\n",
       "  0.08598518371582031,\n",
       "  0.03892350196838379,\n",
       "  -0.02143772691488266,\n",
       "  0.03162992373108864,\n",
       "  0.028219176456332207,\n",
       "  -0.042634300887584686,\n",
       "  0.0885653868317604,\n",
       "  0.048758894205093384,\n",
       "  -0.08924782276153564,\n",
       "  0.02820311300456524,\n",
       "  0.0489613339304924,\n",
       "  -0.01698521338403225,\n",
       "  -0.017068179324269295,\n",
       "  0.04428776726126671,\n",
       "  0.01655430905520916,\n",
       "  -0.015426320023834705,\n",
       "  -0.011243888176977634,\n",
       "  0.0037235459312796593,\n",
       "  0.0014162283623591065,\n",
       "  -0.002069869777187705,\n",
       "  -0.018507929518818855,\n",
       "  -0.02384149841964245,\n",
       "  -0.011366796679794788,\n",
       "  -0.0197732076048851,\n",
       "  -0.016565540805459023,\n",
       "  0.046959660947322845,\n",
       "  -0.015733709558844566,\n",
       "  0.02338704653084278,\n",
       "  0.04443952441215515,\n",
       "  0.027914542704820633,\n",
       "  0.018074339255690575,\n",
       "  0.013430366292595863,\n",
       "  -0.027897769585251808,\n",
       "  0.03517308831214905,\n",
       "  -0.05546797811985016,\n",
       "  0.1284821331501007,\n",
       "  -0.0112619549036026,\n",
       "  -0.0060032750479876995,\n",
       "  0.03736916556954384,\n",
       "  0.0011708879610523582,\n",
       "  -0.02272673323750496,\n",
       "  0.006112938281148672,\n",
       "  -0.03192288801074028,\n",
       "  -0.007867898792028427,\n",
       "  -0.04634872078895569,\n",
       "  -0.03747452795505524,\n",
       "  -0.029103435575962067,\n",
       "  -0.030169367790222168,\n",
       "  0.022661671042442322,\n",
       "  -0.03469301015138626,\n",
       "  0.03122241236269474,\n",
       "  0.052152954041957855,\n",
       "  0.018383804708719254,\n",
       "  -0.017101755365729332,\n",
       "  -0.029125265777111053,\n",
       "  0.026131467893719673,\n",
       "  -0.052113503217697144,\n",
       "  -0.001892694621346891,\n",
       "  -0.03010091930627823,\n",
       "  -0.012288780882954597,\n",
       "  -0.047989483922719955,\n",
       "  -0.07752291858196259,\n",
       "  0.05338605120778084,\n",
       "  0.001996598904952407,\n",
       "  -0.00766550749540329,\n",
       "  0.04822564497590065,\n",
       "  0.008611028082668781,\n",
       "  -0.03173993527889252,\n",
       "  0.001768023706972599,\n",
       "  0.023783359676599503,\n",
       "  -0.005503053776919842,\n",
       "  0.06982202082872391,\n",
       "  -0.03220518305897713,\n",
       "  0.03193647041916847,\n",
       "  -0.04439767822623253,\n",
       "  -0.027567662298679352,\n",
       "  0.02108488604426384,\n",
       "  -0.0009684523101896048,\n",
       "  -0.05873481184244156,\n",
       "  -0.005104685202240944,\n",
       "  0.017676638439297676,\n",
       "  0.07410713285207748,\n",
       "  -0.05026639625430107,\n",
       "  -0.01264579501003027,\n",
       "  0.027096280828118324,\n",
       "  -0.01014963909983635,\n",
       "  0.014283321797847748,\n",
       "  -0.004408403765410185,\n",
       "  0.022124625742435455,\n",
       "  -0.029414968565106392,\n",
       "  -0.010304678231477737,\n",
       "  -0.059467192739248276,\n",
       "  0.005976696498692036,\n",
       "  -0.012508193962275982,\n",
       "  -0.04820496588945389,\n",
       "  -0.020274212583899498,\n",
       "  0.01222971547394991,\n",
       "  0.05176436901092529,\n",
       "  -0.006769976112991571,\n",
       "  0.011509198695421219,\n",
       "  0.023650825023651123,\n",
       "  -0.010712833143770695,\n",
       "  -0.011421414092183113,\n",
       "  -0.023449253290891647,\n",
       "  0.01588442735373974,\n",
       "  -0.022211242467164993,\n",
       "  0.039161279797554016,\n",
       "  0.005278983619064093,\n",
       "  -0.011510060168802738,\n",
       "  0.056815918534994125,\n",
       "  -0.05193701758980751,\n",
       "  -0.045387472957372665,\n",
       "  0.02662048116326332,\n",
       "  0.011780033819377422,\n",
       "  0.017786743119359016,\n",
       "  0.010431002825498581,\n",
       "  -0.09419100731611252,\n",
       "  -0.018109764903783798,\n",
       "  0.04204322770237923,\n",
       "  0.0025771111249923706,\n",
       "  0.009537240490317345,\n",
       "  0.005382445640861988,\n",
       "  0.05661971867084503,\n",
       "  -0.06001831591129303,\n",
       "  0.023912183940410614,\n",
       "  -0.0233023539185524,\n",
       "  0.021899498999118805,\n",
       "  -0.043685805052518845,\n",
       "  0.07035905867815018,\n",
       "  0.01695207878947258,\n",
       "  -0.02105613984167576,\n",
       "  -0.01751464232802391,\n",
       "  -0.0243336483836174,\n",
       "  -0.011546779423952103,\n",
       "  -0.02181261219084263,\n",
       "  -0.05827699974179268,\n",
       "  0.006037797313183546,\n",
       "  0.030158134177327156,\n",
       "  -0.05311911553144455,\n",
       "  -0.02237221598625183,\n",
       "  -0.027055326849222183,\n",
       "  -0.06955167651176453,\n",
       "  0.08622957766056061,\n",
       "  -0.02810513600707054,\n",
       "  0.03729637712240219,\n",
       "  -0.022731689736247063,\n",
       "  -0.03603706508874893,\n",
       "  0.003011007560417056,\n",
       "  -0.05821797996759415,\n",
       "  0.016946403309702873,\n",
       "  -0.03044673427939415,\n",
       "  0.03597752004861832,\n",
       "  -0.016953960061073303,\n",
       "  0.057709768414497375,\n",
       "  -0.003603704273700714,\n",
       "  -0.006611943710595369,\n",
       "  0.03541519120335579,\n",
       "  -0.01175177376717329,\n",
       "  0.0043479567393660545,\n",
       "  0.024213245138525963,\n",
       "  0.01071127038449049,\n",
       "  0.008095230907201767,\n",
       "  -0.009071020409464836,\n",
       "  0.000916356104426086,\n",
       "  -0.025628603994846344,\n",
       "  0.00890535581856966,\n",
       "  0.03191186487674713,\n",
       "  -0.02951943874359131,\n",
       "  0.01859729364514351,\n",
       "  0.02400919422507286,\n",
       "  0.014047820121049881,\n",
       "  -0.05498507246375084,\n",
       "  -0.03968555107712746,\n",
       "  0.015810651704669,\n",
       "  -0.031606901437044144,\n",
       "  0.06502997130155563,\n",
       "  -0.025401242077350616,\n",
       "  -0.02095220610499382,\n",
       "  0.029414242133498192,\n",
       "  -0.012050552293658257,\n",
       "  0.014147341251373291,\n",
       "  0.03242574632167816,\n",
       "  0.01692335121333599,\n",
       "  0.05583979934453964,\n",
       "  -0.0038950848393142223,\n",
       "  0.002931661205366254,\n",
       "  0.018460258841514587,\n",
       "  -0.002148326253518462,\n",
       "  -0.00968870334327221,\n",
       "  0.0006374417571350932,\n",
       "  -0.04347413033246994,\n",
       "  -0.028180530294775963,\n",
       "  -0.01574019156396389,\n",
       "  -0.04516182094812393,\n",
       "  -0.015610129572451115,\n",
       "  0.06332165002822876,\n",
       "  -0.009867540560662746,\n",
       "  0.021551501005887985,\n",
       "  0.02622673474252224,\n",
       "  0.018311452120542526,\n",
       "  0.015021514147520065,\n",
       "  -0.0013287586625665426,\n",
       "  0.005108452867716551,\n",
       "  -0.009394288994371891,\n",
       "  0.04487167298793793,\n",
       "  -0.02477952279150486,\n",
       "  0.01630435325205326,\n",
       "  0.03865950554609299,\n",
       "  0.0008123326115310192,\n",
       "  -0.04230114817619324,\n",
       "  -0.02623802237212658,\n",
       "  -0.032332710921764374,\n",
       "  0.01238559652119875,\n",
       "  -0.025907540693879128,\n",
       "  -0.006053080316632986,\n",
       "  0.0027294845785945654,\n",
       "  0.01098861638456583,\n",
       "  0.07185958325862885,\n",
       "  -0.0036457909736782312,\n",
       "  0.031207749620079994,\n",
       "  0.018433792516589165,\n",
       "  0.05485217273235321,\n",
       "  0.0035750740207731724,\n",
       "  -0.06099442020058632,\n",
       "  -0.004641029052436352,\n",
       "  0.008328028954565525,\n",
       "  -0.05305977910757065,\n",
       "  -0.06736332923173904,\n",
       "  -0.04991224408149719,\n",
       "  -0.029872633516788483,\n",
       "  0.009803222492337227,\n",
       "  0.0015044166939333081,\n",
       "  0.03132166713476181,\n",
       "  0.013391255401074886,\n",
       "  0.013874094001948833,\n",
       "  0.03321298584342003,\n",
       "  0.014973427169024944,\n",
       "  0.030773574486374855,\n",
       "  0.0031653877813369036,\n",
       "  0.01799429953098297,\n",
       "  -0.012269767001271248,\n",
       "  0.053877100348472595,\n",
       "  0.05828772112727165,\n",
       "  -0.0270958561450243,\n",
       "  0.0288167055696249,\n",
       "  -0.03173334151506424,\n",
       "  -0.010051537305116653,\n",
       "  0.003845199476927519,\n",
       "  0.043623924255371094,\n",
       "  0.02748892828822136,\n",
       "  0.0237132478505373,\n",
       "  0.013816402293741703,\n",
       "  -0.024316983297467232,\n",
       "  0.01958739012479782,\n",
       "  0.013778183609247208,\n",
       "  0.028209349140524864,\n",
       "  -0.05628199875354767,\n",
       "  -0.010384863242506981,\n",
       "  -0.0073036327958106995,\n",
       "  0.001081991009414196,\n",
       "  -0.03793539106845856,\n",
       "  -0.003267465392127633,\n",
       "  0.04517139121890068,\n",
       "  0.02595013938844204,\n",
       "  0.06158730387687683,\n",
       "  -0.017308782786130905,\n",
       "  0.0467580147087574,\n",
       "  -0.05315803363919258,\n",
       "  -0.015326661989092827,\n",
       "  -0.04360968992114067,\n",
       "  0.01765422895550728,\n",
       "  0.03723125159740448,\n",
       "  -0.012347943149507046,\n",
       "  0.010245438665151596,\n",
       "  -0.035331033170223236,\n",
       "  0.009715236723423004,\n",
       "  0.05699315294623375,\n",
       "  -0.012192249298095703,\n",
       "  0.07522621005773544,\n",
       "  -0.016124367713928223,\n",
       "  -0.013410014100372791,\n",
       "  0.002872627694159746,\n",
       "  -0.014676802791655064,\n",
       "  0.024777866899967194,\n",
       "  0.021170170977711678,\n",
       "  0.030057592317461967,\n",
       "  0.022470837458968163,\n",
       "  -0.018439706414937973,\n",
       "  -0.004288138821721077,\n",
       "  -0.03677321970462799,\n",
       "  -0.043790917843580246,\n",
       "  0.011872831732034683,\n",
       "  -0.04845314100384712,\n",
       "  -0.03689954802393913,\n",
       "  0.015063218772411346,\n",
       "  0.0032950695604085922,\n",
       "  -0.04325691983103752,\n",
       "  -0.003543703816831112,\n",
       "  -0.020510079339146614,\n",
       "  -0.0023053353652358055,\n",
       "  0.03271182253956795,\n",
       "  0.01312529668211937,\n",
       "  0.053059522062540054,\n",
       "  -0.004046352580189705,\n",
       "  0.027937589213252068,\n",
       "  -0.010441027581691742,\n",
       "  0.053917791694402695,\n",
       "  -0.011930958367884159,\n",
       "  0.02229945734143257,\n",
       "  -0.01105576939880848,\n",
       "  -0.029969260096549988,\n",
       "  0.015483230352401733,\n",
       "  0.04963149502873421,\n",
       "  0.01512794941663742,\n",
       "  0.027856286615133286,\n",
       "  0.004994671326130629,\n",
       "  0.015729377046227455,\n",
       "  -0.009298422373831272,\n",
       "  -0.047744330018758774,\n",
       "  0.052499063313007355,\n",
       "  -0.03900962695479393,\n",
       "  0.0119877178221941,\n",
       "  -0.05106296390295029,\n",
       "  0.000844260910525918,\n",
       "  -0.02019749954342842,\n",
       "  -0.0026802895590662956,\n",
       "  0.007021980360150337,\n",
       "  0.010375991463661194,\n",
       "  -0.025589680299162865,\n",
       "  -0.01580154336988926,\n",
       "  0.010773574002087116,\n",
       "  -0.014496106654405594,\n",
       "  -0.02087497152388096,\n",
       "  -0.057927992194890976,\n",
       "  0.0072604515589773655,\n",
       "  0.012397686950862408,\n",
       "  -0.037448443472385406,\n",
       "  -0.06481225788593292,\n",
       "  -0.05978401377797127,\n",
       "  0.01561176311224699,\n",
       "  0.0384487584233284,\n",
       "  -0.049104996025562286,\n",
       "  0.03701210767030716,\n",
       "  0.020394954830408096,\n",
       "  0.006168561056256294,\n",
       "  0.05434625223278999,\n",
       "  -0.023405957967042923,\n",
       "  -0.04449992626905441,\n",
       "  -0.0010521061485633254,\n",
       "  -0.0056085241958498955,\n",
       "  -0.009393863379955292,\n",
       "  0.012947646901011467,\n",
       "  -0.037476133555173874,\n",
       "  -0.020197831094264984,\n",
       "  0.0017979128751903772,\n",
       "  0.02365189604461193,\n",
       "  0.006311794742941856,\n",
       "  0.052533287554979324,\n",
       "  -0.027901306748390198,\n",
       "  -0.05792554095387459,\n",
       "  0.036973193287849426,\n",
       "  -0.02253880351781845,\n",
       "  0.043659791350364685,\n",
       "  0.00278953998349607,\n",
       "  -0.004199530929327011,\n",
       "  -0.02277201972901821,\n",
       "  0.03168804198503494,\n",
       "  0.01490249577909708,\n",
       "  -0.017331967130303383,\n",
       "  -0.005371608771383762,\n",
       "  0.0051161153241992,\n",
       "  0.03933539614081383,\n",
       "  0.01730162277817726,\n",
       "  0.04049309715628624,\n",
       "  -0.00498223677277565,\n",
       "  -0.04275231063365936,\n",
       "  0.0347801074385643,\n",
       "  0.016671892255544662,\n",
       "  0.004487157333642244,\n",
       "  0.027356741949915886,\n",
       "  -0.018926911056041718,\n",
       "  0.02159317024052143,\n",
       "  0.011741427704691887,\n",
       "  0.04277748242020607,\n",
       "  -0.012997927144169807,\n",
       "  0.05179830268025398,\n",
       "  0.020855698734521866,\n",
       "  -0.02667395770549774,\n",
       "  -0.036289867013692856,\n",
       "  -0.0400526262819767,\n",
       "  0.03362767770886421,\n",
       "  0.06008772552013397,\n",
       "  0.05045640468597412,\n",
       "  -0.009735297411680222,\n",
       "  -0.0943915843963623,\n",
       "  0.0307534858584404,\n",
       "  -0.0006148103275336325,\n",
       "  0.04877247288823128,\n",
       "  0.035489410161972046,\n",
       "  -0.012281601317226887,\n",
       "  0.09890640527009964,\n",
       "  -0.014090271666646004,\n",
       "  0.024646375328302383,\n",
       "  0.022597137838602066,\n",
       "  0.0674906075000763,\n",
       "  0.012036855332553387,\n",
       "  -0.012856517918407917,\n",
       "  0.028531549498438835,\n",
       "  -0.06081292778253555,\n",
       "  -0.0008234400884248316,\n",
       "  -0.0008183119934983552,\n",
       "  -0.039910927414894104,\n",
       "  -0.009644252248108387,\n",
       "  -0.038737520575523376,\n",
       "  0.017798442393541336,\n",
       "  0.08049352467060089,\n",
       "  0.003564476268365979,\n",
       "  0.0063497163355350494,\n",
       "  0.040587931871414185,\n",
       "  0.0023127642925828695,\n",
       "  -0.046195995062589645,\n",
       "  -0.007579298689961433,\n",
       "  -0.014091845601797104,\n",
       "  -0.02868395857512951,\n",
       "  0.04969640448689461,\n",
       "  0.020348908379673958,\n",
       "  0.013909829780459404,\n",
       "  -0.043659672141075134,\n",
       "  -0.031509436666965485,\n",
       "  -0.04301472380757332,\n",
       "  -0.02940523996949196,\n",
       "  0.04779009148478508,\n",
       "  0.010702697560191154,\n",
       "  -0.008380900137126446,\n",
       "  0.01532717328518629,\n",
       "  0.0025753185618668795,\n",
       "  0.03384185582399368,\n",
       "  0.030054708942770958,\n",
       "  -0.009382524527609348,\n",
       "  0.04055950418114662,\n",
       "  -0.051176730543375015,\n",
       "  0.0009917390998452902,\n",
       "  -0.025614319369196892,\n",
       "  0.02761932834982872,\n",
       "  0.03859172761440277,\n",
       "  0.010631609708070755,\n",
       "  0.031095923855900764,\n",
       "  0.050465598702430725,\n",
       "  -0.015154273249208927,\n",
       "  -0.030093902722001076,\n",
       "  0.001283131306990981,\n",
       "  -0.015485785901546478,\n",
       "  0.021016757935285568,\n",
       "  -0.07293471693992615,\n",
       "  -0.06379377096891403,\n",
       "  0.05269457772374153,\n",
       "  -0.008030461147427559,\n",
       "  0.02555198408663273,\n",
       "  0.022717023268342018,\n",
       "  0.013258688151836395,\n",
       "  0.025278303772211075,\n",
       "  -0.0959986075758934,\n",
       "  0.015347741544246674,\n",
       "  0.017089704051613808,\n",
       "  -0.052473463118076324,\n",
       "  0.007565115578472614,\n",
       "  0.03812175989151001,\n",
       "  -0.040537379682064056,\n",
       "  -0.00284092640504241,\n",
       "  -0.009947958402335644,\n",
       "  -0.061156149953603745,\n",
       "  0.0115430923178792,\n",
       "  0.009092139080166817,\n",
       "  -0.05639364570379257,\n",
       "  0.03049647808074951,\n",
       "  -0.03132138028740883,\n",
       "  0.03674125298857689,\n",
       "  0.03158072382211685,\n",
       "  -0.06252218037843704,\n",
       "  0.013933462090790272,\n",
       "  0.006052760407328606,\n",
       "  -0.0010163304395973682,\n",
       "  -0.0015172440325841308,\n",
       "  0.030293552204966545,\n",
       "  0.006051312200725079,\n",
       "  0.02180284820497036,\n",
       "  0.023251378908753395,\n",
       "  0.0032416610047221184,\n",
       "  0.011617998592555523,\n",
       "  -0.0041241697035729885,\n",
       "  0.01732632890343666,\n",
       "  -0.005065687000751495,\n",
       "  -0.019746212288737297,\n",
       "  -0.022921061143279076,\n",
       "  0.0027627076487988234,\n",
       "  -0.11131534725427628,\n",
       "  0.011743836104869843,\n",
       "  -0.0790976956486702,\n",
       "  0.007074638269841671,\n",
       "  -0.018105996772646904,\n",
       "  0.03971526026725769,\n",
       "  -0.044448189437389374,\n",
       "  -0.004304907284677029,\n",
       "  -0.035842571407556534,\n",
       "  0.00013029976980760694,\n",
       "  -0.01022109854966402,\n",
       "  -0.017769426107406616,\n",
       "  0.034327756613492966,\n",
       "  0.09469223767518997,\n",
       "  -0.020723579451441765,\n",
       "  -0.0102561479434371,\n",
       "  -0.03119329921901226,\n",
       "  -0.0112233841791749,\n",
       "  -0.019751107320189476,\n",
       "  0.017695119604468346,\n",
       "  0.04397230222821236,\n",
       "  0.04024931788444519,\n",
       "  -0.04771008342504501,\n",
       "  0.02876295894384384,\n",
       "  0.025717608630657196,\n",
       "  -0.006066405214369297,\n",
       "  -0.0023153710644692183,\n",
       "  0.013874715194106102,\n",
       "  -0.05492113530635834,\n",
       "  -0.08978188037872314,\n",
       "  -0.06358234584331512,\n",
       "  -0.0007658522808924317,\n",
       "  -0.04651898145675659,\n",
       "  -0.002035488374531269,\n",
       "  0.014766364358365536,\n",
       "  -0.013998577371239662,\n",
       "  0.027173884212970734,\n",
       "  -0.05244337394833565,\n",
       "  -0.00819766242057085,\n",
       "  0.042641427367925644,\n",
       "  -0.015341522172093391,\n",
       "  -0.009929703548550606,\n",
       "  -0.026121623814105988,\n",
       "  0.03824213519692421,\n",
       "  -0.052159469574689865,\n",
       "  -0.02171228639781475,\n",
       "  -0.01246955431997776,\n",
       "  -0.011763479560613632,\n",
       "  -0.04113392159342766,\n",
       "  -0.0619523786008358,\n",
       "  -0.06267049163579941,\n",
       "  0.010677997954189777,\n",
       "  0.037554558366537094,\n",
       "  0.07832998782396317,\n",
       "  -0.06904743611812592,\n",
       "  0.033381663262844086,\n",
       "  0.09780395030975342,\n",
       "  -0.01002598088234663,\n",
       "  0.043185651302337646,\n",
       "  -0.01249961368739605,\n",
       "  -0.009890202432870865,\n",
       "  -0.028410525992512703,\n",
       "  0.012895614840090275,\n",
       "  0.0016509972047060728,\n",
       "  -0.01111752912402153,\n",
       "  0.02983814664185047,\n",
       "  -0.0528205931186676,\n",
       "  0.07117057591676712,\n",
       "  -0.020613353699445724,\n",
       "  -0.0632481649518013,\n",
       "  0.00742728915065527,\n",
       "  0.0064440276473760605,\n",
       "  -0.04852436110377312,\n",
       "  0.07205727696418762,\n",
       "  -0.03532883897423744,\n",
       "  0.03643650934100151,\n",
       "  0.012760255485773087,\n",
       "  -0.028259893879294395,\n",
       "  -0.04551166668534279,\n",
       "  0.024059660732746124,\n",
       "  0.03287394717335701,\n",
       "  -0.05157816782593727,\n",
       "  0.006953654810786247,\n",
       "  -0.02628953754901886,\n",
       "  -0.00826314277946949,\n",
       "  -0.0648580864071846,\n",
       "  0.034352656453847885,\n",
       "  -0.038537390530109406,\n",
       "  0.013676013797521591,\n",
       "  0.04041499271988869,\n",
       "  0.04610541835427284,\n",
       "  0.024043874815106392,\n",
       "  -0.030052829533815384,\n",
       "  -0.01402998249977827,\n",
       "  -0.009298667311668396,\n",
       "  0.037733521312475204,\n",
       "  -0.009731649421155453,\n",
       "  0.04256987199187279,\n",
       "  0.07429340481758118,\n",
       "  0.014780022203922272,\n",
       "  -0.02596099302172661,\n",
       "  0.08479147404432297,\n",
       "  0.11107120662927628,\n",
       "  0.008806531317532063,\n",
       "  -0.04956716299057007,\n",
       "  -0.04125458374619484,\n",
       "  0.0027583218179643154,\n",
       "  0.0019896836020052433,\n",
       "  -0.020289286971092224,\n",
       "  -0.008946695365011692,\n",
       "  -0.014245175756514072,\n",
       "  -0.007200656924396753,\n",
       "  -0.0008821827941574156,\n",
       "  0.030396774411201477,\n",
       "  0.012970414943993092,\n",
       "  0.05031302943825722,\n",
       "  -0.014651913195848465,\n",
       "  -0.008435471914708614,\n",
       "  -0.03848913311958313,\n",
       "  -0.04469872638583183,\n",
       "  -0.03488361835479736,\n",
       "  0.0404793955385685,\n",
       "  0.010832402855157852,\n",
       "  0.0047727543860673904,\n",
       "  -0.014770402573049068,\n",
       "  0.054265279322862625,\n",
       "  0.02515033446252346,\n",
       "  -0.015603063628077507,\n",
       "  0.010345694608986378,\n",
       "  0.016136202961206436,\n",
       "  0.005340388510376215,\n",
       "  0.0406016930937767,\n",
       "  -0.008082729764282703,\n",
       "  0.0011988083133473992,\n",
       "  -0.03255651518702507,\n",
       "  0.0013537670020014048,\n",
       "  0.008153960108757019,\n",
       "  1.3948902960692067e-05,\n",
       "  -0.0048060063272714615,\n",
       "  -0.01509962510317564,\n",
       "  -0.03719966486096382,\n",
       "  -0.07313588261604309,\n",
       "  -0.056396856904029846,\n",
       "  -0.010542121715843678,\n",
       "  -0.014066756702959538,\n",
       "  -0.006864011753350496,\n",
       "  0.05472152680158615,\n",
       "  -0.005875575356185436,\n",
       "  0.004761518444865942,\n",
       "  0.0342254675924778,\n",
       "  0.08865713328123093,\n",
       "  0.02534467540681362,\n",
       "  0.037766799330711365,\n",
       "  -3.2612944778520614e-05,\n",
       "  0.025074796751141548,\n",
       "  0.006145231891423464,\n",
       "  -0.0038826207164674997,\n",
       "  0.003535939147695899,\n",
       "  -0.03929484263062477,\n",
       "  -0.0026724166236817837,\n",
       "  -0.08220482617616653,\n",
       "  -0.04936820641160011,\n",
       "  0.01585189625620842,\n",
       "  -0.020449083298444748,\n",
       "  0.0649440810084343,\n",
       "  0.04855216294527054,\n",
       "  0.016499225050210953,\n",
       "  -0.00421437481418252,\n",
       "  -0.0929708257317543,\n",
       "  -0.02817470021545887,\n",
       "  -0.006597709842026234,\n",
       "  0.07319852709770203,\n",
       "  -0.036810483783483505,\n",
       "  -0.035332661122083664,\n",
       "  0.010689495131373405,\n",
       "  -0.005180709064006805,\n",
       "  -0.04582243412733078,\n",
       "  0.03112359531223774,\n",
       "  -0.043734196573495865,\n",
       "  0.0007384519558399916,\n",
       "  0.035094112157821655,\n",
       "  -0.016574939712882042,\n",
       "  -0.05398312583565712,\n",
       "  -0.0380098894238472,\n",
       "  0.03665965422987938,\n",
       "  -0.013240250758826733,\n",
       "  0.03912333771586418,\n",
       "  -0.020858818665146828,\n",
       "  -0.01758481375873089,\n",
       "  -0.06325244903564453,\n",
       "  -0.013302386738359928,\n",
       "  -0.04202921316027641,\n",
       "  0.036253511905670166,\n",
       "  0.026183487847447395,\n",
       "  0.008775843307375908,\n",
       "  -0.035076938569545746,\n",
       "  -0.030515776947140694,\n",
       "  -0.02293042466044426,\n",
       "  0.044000294059515,\n",
       "  -0.0008334745652973652,\n",
       "  -0.046042997390031815,\n",
       "  0.015346315689384937,\n",
       "  -0.040891848504543304,\n",
       "  -0.018914751708507538,\n",
       "  0.029372362419962883,\n",
       "  -0.014785930514335632,\n",
       "  0.027099691331386566,\n",
       "  -0.003008318832144141,\n",
       "  0.047032926231622696,\n",
       "  0.12756061553955078,\n",
       "  0.012376431375741959,\n",
       "  0.00205716653726995,\n",
       "  -0.023623548448085785,\n",
       "  0.06421734392642975,\n",
       "  0.0037240979727357626,\n",
       "  -0.016818298026919365,\n",
       "  0.008925538510084152,\n",
       "  -0.04735887423157692,\n",
       "  -0.019141823053359985],\n",
       " 'text': 'Scaling Laws for Neural Language Models\\n\\nJared Kaplan ∗\\n\\nJohns Hopkins University, OpenAI\\n\\njaredk@jhu.edu\\n\\nSam McCandlish∗\\n\\nOpenAI\\n\\nsam@openai.com\\n\\nTom Henighan\\n\\nTom B. Brown\\n\\nBenjamin Chess\\n\\nRewon Child\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nhenighan@openai.com\\n\\ntom@openai.com\\n\\nbchess@openai.com\\n\\nrewon@openai.com\\n\\nScott Gray\\n\\nOpenAI\\n\\nAlec Radford\\n\\nOpenAI\\n\\nJeffrey Wu\\n\\nOpenAI\\n\\nDario Amodei\\n\\nOpenAI\\n\\nscott@openai.com\\n\\nalec@openai.com\\n\\njeffwu@openai.com\\n\\ndamodei@openai.com\\n\\nAbstract\\n\\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\\nused for training, with some trends spanning more than seven orders of magnitude. Other\\narchitectural details such as network width or depth have minimal effects within a wide\\nrange. Simple equations govern the dependence of overﬁtting on model/dataset size and the\\ndependence of training speed on model size. These relationships allow us to determine the\\noptimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-\\nefﬁcient, such that optimally compute-efﬁcient training involves training very large models\\non a relatively modest amount of data and stopping signiﬁcantly before convergence.\\n\\n0\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n3\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n1\\n6\\n3\\n8\\n0\\n.\\n1\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n∗Equal contribution.\\n\\nContributions:\\nJared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex-\\nperiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\\nguidance throughout the project.\\n\\n \\n \\n \\n \\n \\n \\n\\x0cContents\\n\\n1 Introduction\\n\\n2 Background and Methods\\n\\n3 Empirical Results and Basic Power Laws'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.store[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M1) - 4861 MiB free\n",
      "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from models/Llama-3.2-1B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models Meta Llama Llama 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = models-meta-llama-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = ./Llama-3.2-1B-Instruct-GGUF_imatrix.dat\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 68\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q4_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \n",
      "llm_load_print_meta: general.name     = Models Meta Llama Llama 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q6_K) (and 162 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/17 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   762.81 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x12ace3100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x14a10a3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x14a204080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x14a604400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x14a2042e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x13af20010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x14a109dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x14a10b5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x13af2e8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x14a10b8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x14a204540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x13af2eb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x13af2f080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x13af2f5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x14a204b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x14a6048e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x13af2fad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x14a604dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x14a204ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x14a205760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x14a6052a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13af2ffb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x13af30490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x13af30970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x13af30bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a2059c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x14a6059e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a205c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a205e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13af30e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x13af31090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x13af312f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a605d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13af31550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a205250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a2060e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a109140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a206340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13af317b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13af31a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a2065a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a10c3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a206800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13af31dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a206a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a10d4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a206cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a206f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a207180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a10caa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a2073e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x13af32320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x14a10ebc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x14a606500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x14a10f4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a207640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a6070f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a10e7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a10fe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a110680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13af32ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a207ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13af336b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a607ab0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a207d30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13af33ff0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a208560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a110fc0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a208eb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13af34930 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a608400 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a111560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13af35070 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13af35480 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a608ba0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a608ec0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a112840 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a112bc0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a609300 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a112ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a609920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a113880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a209800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13af360e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a609b80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13af36340 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a60a600 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13af36ab0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a20a3f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13af36d10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13af375e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13af38000 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a20a650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a20ab40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a20b9b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a60c1d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a20c3c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a20cd80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a20d720 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a20e110 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13af389b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13af393b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a114b80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a20eb10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13af3a700 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13af3af90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13af3b940 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a115640 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a115c80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a20f4b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a116900 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a20fe10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a210770 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13af3c2b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13af3cbe0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13af3d530 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a117170 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13af3df20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13af3e8f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13af3f290 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13af3fc60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a117b00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a2109d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a60acf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a118c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a1197f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13af40630 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a60d410 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a119a50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a60d7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a60e1a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13af419a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a60e400 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c2d05f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13af43240 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a60fb70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a211240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae2e660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a119ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c333e00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a211c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a211ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a212650 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a213490 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a11b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a60fe90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a213c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a6109f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x14a11aa00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x14a11c830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a610c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a611220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x14a6125a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x14a11c020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a612ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x14a613850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c2c02d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c2c1030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c2c0b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13af436a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13af43900 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13af44960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a11dc80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c334470 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c13b580 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a214fc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a2152e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c2c21e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c12fb50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a215b50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13af442a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c2c2ad0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c334eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c12fdb0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a216670 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a2173e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13af05b40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a11e2d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13af05da0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a11ee30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a217640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a614330 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13af070a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13af07930 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a11fc40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a614590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a217da0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13af07b90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13af084a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13af09010 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a120710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a81c650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a120fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a81d060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a614ec0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a615810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a218450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a615a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a81db80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a120b30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a121ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a21a0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a81dde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a219bd0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a218c90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a1225b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a21ba60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a81e810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a21c370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a123850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a81f130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a81fc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a124530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a820ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a8213b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a21c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a616e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x13a820610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x13a822840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x13a823390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x13a823ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x14a21da70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x13a8247e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a617660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a6178c0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   254.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 258 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': './Llama-3.2-1B-Instruct-GGUF_imatrix.dat', 'quantize.imatrix.chunks_count': '68', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '2048', 'llama.vocab_size': '128256', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.attention.value_length': '64', 'llama.attention.head_count': '32', 'llama.attention.key_length': '64', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '15', 'llama.block_count': '16', 'general.size_label': '1B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '8192', 'general.quantization_version': '2', 'llama.rope.dimension_count': '64', 'general.license': 'llama3.2', 'quantize.imatrix.entries_count': '112', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'models-meta-llama-Llama-3.2', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': 'group_40.txt', 'general.name': 'Models Meta Llama Llama 3.2 1B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"models/Llama-3.2-1B-Instruct.Q4_K_M.gguf\", n_ctx=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    9048.58 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1520 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14882.08 ms /  1758 tokens\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an assistant that answers user questions about a collection of movie screenplays.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Use the following pieces of context to answer the user question.\n",
    "You must only use the facts from the context to answer.\n",
    "If the answer cannot be found in the context, say that you don't have enough information to answer the question and provide any relevant facts found in the context.\n",
    "Don't address \\\"the context\\\" explicitly in your answer, answer the question like it's your own knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"what is the scalling law?\"\n",
    "\n",
    "\n",
    "# Embed the user's question\n",
    "embed_res = embed.text(\n",
    "    texts=[question],\n",
    "    model=\"nomic-embed-text-v1.5\",\n",
    "    task_type=\"search_query\",\n",
    "    inference_mode=\"local\",\n",
    ")\n",
    "query_vector = embed_res[\"embeddings\"][0]\n",
    "\n",
    "# Find the most relevant chunks in our vector store using semantic search\n",
    "chunks = vector_store.query(query_vector, top_k=3)\n",
    "\n",
    "# Prepare the context and prompt, and generate an answer with the LLM\n",
    "context = \"\\n\\n---\\n\\n\".join([chunk[\"text\"] for chunk in chunks]) + \"\\n\\n---\"\n",
    "user_message = USER_PROMPT.format(context=context, question=question)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "chat_completion = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\\nback on drafts of this work.\\n\\n19\\n\\n\\x0cAppendices\\n\\nA Summary of Power Laws\\n\\nFor easier reference, we provide a summary below of the key trends described throughout the paper.\\n\\nParameters Data Compute\\n\\nBatch Size Equation\\n\\nN\\n\\n∞\\n\\nOptimal\\n\\n∞\\n\\nD\\n\\n∞\\n\\n∞\\n\\nEarly Stop\\n\\nC\\n\\nFixed\\n\\nFixed\\n\\nFixed\\n\\nL (N ) = (Nc/N )αN\\nL (D) = (Dc/D)αD\\nL (C) = (Cc/C)αC (naive)\\n\\nNopt\\n\\nDopt\\n\\nCmin\\n\\nN\\n\\nN\\n\\nD\\n\\n∞\\n\\nEarly Stop\\n\\nB (cid:28) Bcrit L (Cmin) = (cid:0)C min\\nc\\n(cid:0) Nc\\nN\\n\\nL (N, D) =\\n\\nFixed\\n\\n(cid:20)\\n\\n(cid:1)αmin\\n\\nC\\n\\n(cid:21)αD\\n\\n/Cmin\\n(cid:1) αN\\nαD + Dc\\nD\\n(cid:16)\\n\\n(cid:1)αN +\\n\\n(cid:17)αS\\n\\nSc\\nSmin(S,B)\\n\\nS steps\\n\\nB\\n\\nL (N, S) = (cid:0) Nc\\n\\nN\\n\\nTable 4\\n\\nThe empirical ﬁtted values for these trends are:\\n\\nPower Law\\n\\nScale (tokenization-dependent)\\n\\nαN = 0.076\\n\\nαD = 0.095\\n\\nNc = 8.8 × 1013 params (non-embed)\\nDc = 5.4 × 1013 tokens\\nCc = 1.6 × 107 PF-days\\n\\nαC = 0.057\\nC = 0.050 C min\\nαmin\\nαB = 0.21\\n\\nαS = 0.76\\n\\nc = 3.1 × 108 PF-days\\n\\nB∗ = 2.1 × 108 tokens\\nSc = 2.1 × 103 steps\\n\\nThe optimal parameters for compute efﬁcient training are given by:\\n\\nTable 5\\n\\nCompute-Efﬁcient Value\\n\\nPower Law Scale\\n\\n---\\n\\nThe intersection point is sensitive to the precise power-law parameters\\x0cOne might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\\nsize beyond N ∗ without qualitatively different data requirements, perhaps this means that once we reach\\nC ∗\\nmin and N ∗, we have extracted all of the reliable information available in natural language data. In this\\ninterpretation, L∗ would provide a rough estimate for the entropy-per-token7 of natural language. In this\\nscenario, we would expect the loss trend to level off at or before L∗.\\n\\nWe can guess at the functional form of L(Cmin) as it levels off by considering a version of our training\\ndataset with added noise. For example, we could append a random string of tokens to each context shown\\nto the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise\\nﬂoor L − Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\\npotentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect\\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and\\nmay be meaningful even if it occurs after the leveling off.\\n\\n7 Related Work\\n\\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\\nnumber of relevant features in the data.\\n\\n---\\n\\nScaling Laws for Neural Language Models\\n\\nJared Kaplan ∗\\n\\nJohns Hopkins University, OpenAI\\n\\njaredk@jhu.edu\\n\\nSam McCandlish∗\\n\\nOpenAI\\n\\nsam@openai.com\\n\\nTom Henighan\\n\\nTom B. Brown\\n\\nBenjamin Chess\\n\\nRewon Child\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nOpenAI\\n\\nhenighan@openai.com\\n\\ntom@openai.com\\n\\nbchess@openai.com\\n\\nrewon@openai.com\\n\\nScott Gray\\n\\nOpenAI\\n\\nAlec Radford\\n\\nOpenAI\\n\\nJeffrey Wu\\n\\nOpenAI\\n\\nDario Amodei\\n\\nOpenAI\\n\\nscott@openai.com\\n\\nalec@openai.com\\n\\njeffwu@openai.com\\n\\ndamodei@openai.com\\n\\nAbstract\\n\\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\\nused for training, with some trends spanning more than seven orders of magnitude. Other\\narchitectural details such as network width or depth have minimal effects within a wide\\nrange. Simple equations govern the dependence of overﬁtting on model/dataset size and the\\ndependence of training speed on model size. These relationships allow us to determine the\\noptimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-\\nefﬁcient, such that optimally compute-efﬁcient training involves training very large models\\non a relatively modest amount of data and stopping signiﬁcantly before convergence.\\n\\n0\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n3\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n1\\n6\\n3\\n8\\n0\\n.\\n1\\n0\\n0\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n∗Equal contribution.\\n\\nContributions:\\nJared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex-\\nperiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\\nguidance throughout the project.\\n\\n \\n \\n \\n \\n \\n \\n\\x0cContents\\n\\n1 Introduction\\n\\n2 Background and Methods\\n\\n3 Empirical Results and Basic Power Laws\\n\\n---'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'The scaling law mentioned in the context is a power-law relationship between the model size, dataset size, and the amount of compute used for training. Specifically, the scaling law is described as:\\n\\nL(Cmin) = (cid:0)C min\\nc\\n(cid:0) Nc\\nN\\n\\nL (N, D) =\\n\\nFixed\\n\\n(cid:20)\\n\\n(cid:1)αmin\\n\\nC\\n\\n(cid:21)αD\\n\\n/Cmin\\n(cid:1) αN\\nαD + Dc\\nD\\n(cid:16)\\n\\n(cid:1)αN +\\n\\n(cid:17)αS\\n\\nSc\\nSmin(S,B)\\n\\nS steps\\n\\nB\\n\\nL (N, S) = (cid:0) Nc\\n\\nN\\n\\nThis scaling law indicates that as the model size (N) increases, the dataset size (D) must also increase to maintain a constant loss (L) per step (S). The optimal model size (N*) is achieved when the loss per step (L) is minimized, and the scaling law provides a way to determine this optimal model size based on the model size, dataset size, and compute budget.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion[\"choices\"][0][\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-from-scratch-R2iSrcyl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
